\section{Comparison with Newton's method}

Before we start the anlaysis of our Method we shortly see what happens if we apply Newton's method on the analytical form of the \MA equation \ref{MA eq}.
Let $F$ be the function such that its root solves the \MA equation, i.e. 
\[
	F(u) = \mydet{D^2 u} -f
\]
Applying Newton's method on $F(u) =0$ we have
\begin{align}
	DF[u^n](u^{i+1}-u^i) = -F[u^i]
\end{align}
where $DF[u]$ denotes the G\^ateaux derivative. We derived the G\^ateaux derivative $DF[u] v = \mycof{D^2 u}:D^2v$ already in theorem \ref{thm: linearisation} leading us to the Newton iteration
\begin{align}
	\mycof{D^2 u^i}:D^2\left(u^{i+1}-u^i\right) &= -\mydet{D^2 u^i}+f \nonumber \\
	\Leftrightarrow \qquad \qquad  \mycof{D^2 u^i}:D^2(u^{i+1}) &= -\mydet{D^2 u^i} +f  +\mycof{D^2 u^i}:D^2(u^i). \label{eq: Newton iteration pre}
\end{align}

Similar to the derivation of the fixed point iteration we can apply Lemma \ref{la: An application of the divergernce product rule} to rewrite \eqref{eq: Newton iteration pre} and we have the problem
\begin{align}
	\nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i+1} \right) &= -\mydet {D^2u^i} +f+\nabla \cdot \left( \cof(D^2 u^i) \nabla u^{i} \right)  \textnormal{ in } \Omega,  \label{eq: Newton iteration}\\
	u^{i+1} &= g \textnormal{ on } \partial \Omega .
\end{align}

Hence, considering once again the fact 
\[
\nabla \cdot \left( \mycof {D^2 u } \nabla v \right)
\stackrel{La.\ref{la: An application of the divergernce product rule}}=\nabla \cdot {}\mycof{D^2 u}:D^2u
=\frac 1 2 \mydet{D^2u}.
\]
we can see our method as a variant of Newton's method. 

In \cite{Awanou2014} Awanou analysed the similar iteration process
\begin{align}
	\nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i+1} \right) &= \nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i} \right) + f - \operatorname{det} (D^2u^i) \textnormal{ in } \Omega,  \label{eq: Awanout eq}\\
	u^{i+1} &= g \textnormal{ on } \partial \Omega.
\end{align}
showing convergence for the analytical solution $u$ and a sufficent close $u^0$. 
In a earlier work \cite{Awanou2010} Awanou examined a discrete Version of a vanishing moment method, herein he mentioned a method he calls Newton's method defined by
\[
	\int_{\Omega} [\mycof{ D^2 u_h^i} Du_h^{i+1}] \cdot Dv_h = -	\int_{\Omega} f v_h + \frac 1 2 \int_{\Omega} [\mycof{ D^2 u_h^i} Du_h^{i}] \cdot Dv_h \; \forall v_h \in V_h \cap H^1_0 (\Omega)  \label{eq: Awanout eq2}.
\]
And indeed this is the variational form of \eqref{eq: Newton iteration}
His chosen trial space were piecewise polynomials contained in $C^1(\Omega)$. He claims that this ansatz breaks down for problems with non-smooth solutions, in his numerical results he cites test \ref{test sqrt} as an example where Newton's method diverges.

\todo{existence theory}
\section{Comparison with the $C^0$ penalty method}
It is also interesting to compare our linearisation with the linearisations of the two other methods.
Consider a Newton step made in the $C^0$ penalty method (cf. \ref{sec: Brenner method}).
\begin{align}
	&\int_\Omega \cofHess {u^i} \nabla (u^{i+1}-u^i) \nabla v - \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla (u^{i+1}-u^i) \cdot \mathbf n \; v \nonumber \\
	&- \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla v \cdot \mathbf n (u^{i+1}-u^i) + \sigma \sum_{e \in \edgesb} \frac 1 {|e|} \int_e v (u^{i+1}-u^i)\nonumber \\
	=&-\int_\Omega \left(f - \detHess{u^i)}\right) v  
			- \sum_{e \in \edgesi} \int_e \jump { \average{\cofHess {u^i}} \nabla {u^i} } v \nonumber \\
&	- \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla v \cdot \mathbf n \; (u^i-g) - \sigma \sum_{e \in \edgesb} \frac 1 {|e|} \int_e v (u^{i}-g) \label{eq: a newton step Brenner}
\end{align}
To compare both linearisations we first reorder and remove cancelling terms in \eqref{eq: a newton step Brenner}.
\begin{align}
	&\int_\Omega \cofHess {u^i} \nabla u^{i+1} \nabla v \\
	&- \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla u^{i+1} \cdot \mathbf n \; v 
		- \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla v \cdot \mathbf n \; u^{i+1} \\
	=
	&-\int_\Omega \left(f - \detHess{u^i)}\right) v \\
	&+ \sum_{e \in \edgesb} \int_e \cofHess {u^i} \nabla v \cdot \mathbf n \; g 
	+\sigma \sum_{e \in \edgesb} \frac 1 {|e|} \int_e v (-u^{i}+g) \label{eq: ordered newton step Brenner}
\end{align}

\section{Conclusion}

We saw the performances of three DG methods solving PDEs of \MA type.
The first method, introduced by Brenner et alter yields a good result for the smooth test case but failed for every other test case, even for test case \ref{test singularity} that has a solution in $C^1(\Omega)$. Hence, as the developer indicate in their paper it is only suitable for finding classical solutions.
Brenner et alter used a vanishing moment method to find a proper initial guess. This seems rather costly and our numerical results show the method could be improved using a nested iterations and a simple linear PDE for the first initial guess.

The results Neilan showed in his paper for our second examined method look very promising.
Note, that this method has if polynomial degree of $V_h$ and $\Sigma_h$ are chosen equally has as five times as many degree of freedoms compared to both other methods.

The last presented method turned out to be a variant of an classical damped Newton approach. 
\todo{verbesserung mit modifizierter cofactor matrix}
The classical approach is known to fail for elements which are not at least contained in $C^1(\Omega)$. Unfortunately the fixed point iteration do also diverge for finer grids. Yet it may serve for an initial guess during a multilevel method. For big grid widths in our test cases the $L2$ error even decreased for problems which do not have a classical solution.

Comparing the three methods for the smooth test case the first and second method produce similar convergence orders while the first has less degree of freedoms. The Picard type method yields even for the first four refinement a much smaller convergence rate.

We saw that recent DG methods perform well on problems with classical solution, but have problems with problems with only viscocity solution or Aleksandrov solutions. Most rely on Newton's method to solve their nonlinear system and how to provide good initial guesses is not answered yet.
My numerical results show that a nested iteration approach often does not provide suitable starting points for further refinements.

\section{Perspective}
Handling fully nonlinear PDEs is a complex domain and results on this domain are very unsatisfactory when compared to the linear case. Even when restrict ourselves to the prototype of nonlinear PDEs, the \MA equation, the theory is far from being complete for both the analytical and the numerical point of view.

Recent DG methods work provably well for classical solutions, but to the author's knowledge there are no proven statement on their performance for viscosity solutions or Aleksandrov solutions. Even in the case we have a classical solution convergence of DG method is only proven for polynomial degrees greater or equal than three. Though most numerical experiment suggest a relaxation it could not be proved yet.

We presented methods for the \MA equation given that the right-hand side only depend on $x$ and the left-hand side only on the determinant of the Hessian. Currently DG methods were often not extended to more general PDEs. It has to be analysed, if  discretisations for right-hand sides depending on $u$ and $\nabla u$ and more complicated left-hand sides may be derived analogously. An example for one of those more complicated left-hand sides is $\mydet {D^2 u +A}$ for $A:\Omega \rightarrow \R^{d \times d}$.
 
The implemented convexification did not support the solution process. Yet, it is worth to address this approach further. Maybe it is useful to convexify intermediate solution only on fine grids.


