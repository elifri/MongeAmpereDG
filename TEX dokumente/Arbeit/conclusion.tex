\section{Conclusion}

In this thesis we discussed DG methods for the \MA equation. In Chapter \ref{ch:TheoreticalBackground} we reviewed PDEs and DG methods, in particular we analysed SIPG methods, their theoretical validation and practical implementation.
Afterwards we introduced the \MA equation, collect information about its main properties and gave an overview over existing numerical methods to solve it. In Chapter \ref{ch:DGMongeAmpere} we described the ideas of the Galerkin methods, and  we presented two methods in more detail: The first developed by Brenner et al. \cite{BGN+2011} and the second introduced by Neilan \cite{Neilan2014}.
The main chapters were given by Chapter \ref{ch:ourMethod} and \ref{ch:NumericalResults}. Herein we first derive a new DG method of Picard type. Since the original derived method does not solve the \MA equation we discuss several improvements to ensure convergence. 
In the section on numerical results we provided performances of three DG methods: The two methods presented in Chapter \ref{ch:DGMongeAmpere} which are proven to converge with optimal error estimates in the case of a smooth solutions and the newly derived Picard iteration to which at the moment no reliable analytical convergence results exist.

The first method, a $C^0$ penalty method, yields a good result for the smooth test case but fails for every other test case, even for the test case that has a solution in $C^1(\Omega)$. Hence, as also the developer indicate in their paper it is only suitable for finding classical solutions.
Brenner et alt. used a vanishing moment method to find a proper initial guess. This seems rather laborious and our numerical results show the method could be improved using a nested iterations and a simple linear PDE for the first initial guess.

The results Neilan shows in his paper for our second examined method look very promising. Even in the non-smooth cases his method converges. However, Section \ref{sec: numerical results neilan} shows the Newton solver for his finite element method has to be chosen very carefully. In particular for many degrees of freedom only a few nonlinear solver converge, for higher polynomial degree no solver solving the nonlinear equation system could be determined.
Note that this method requires a lot of freedom. If the polynomial degree of $V_h$ and $\Sigma_h$ are chosen equally, it has as five times as many degree of freedoms compared to both other methods. However, our numerical results suggests that decreasing the polynomial degree of $\Sigma_h$ results only in a small loss of accuracy.

The last presented method, derived by a Picard linearisation turned out to be a variant of an classical damped Newton approach which is said to fail for elements which are not at least contained in $C^1(\Omega)$.
Unfortunately, the Picard iteration also diverges for finer grids, especially if the solution is not smooth. Introducing several improvements and varying parameters may slow down, but cannot prevent the divergence process. Yet it may serve for an initial guess during a nested iteration method, even in the case the problem has no classical solutions.

Comparing the three methods in the smooth test case the first and second method produce similar convergence orders. % while the first has less degree of freedoms. 
The Picard type method yields even during the first four refinements a smaller convergence rate. 

We can say recent DG methods perform well on problems with classical solution, but we also experienced they have problems if the \MA solution is only a viscocity solution or Aleksandrov solution. Most rely on Newton's method to solve their nonlinear system and how to provide good initial guesses is not answered satisfactorily yet.
Our numerical results indicate that a nested iteration approach often does not provide suitable starting points for further refinements, especially if the method employs a lot of degrees of freedom in every cell.

\section{Perspective}
Handling fully nonlinear PDEs is a complex domain and results on this domain are very unsatisfactory when compared to the linear case. Even when restricting ourselves to the prototype of nonlinear PDEs, the \MA equation, the theory is far from being complete. %, for both the analytical and the numerical point of view.

Recent DG methods work provably well for classical solutions, but to the author's knowledge there are no proven statement on their performance for viscosity solutions or Aleksandrov solutions. Even in the case we have a classical solution convergence of DG method is only proven for polynomial degrees greater or equal than three. Though most numerical experiment suggest a relaxation it could not be proved yet.

The Picard iteration relies on the decoupling of the solution into two functions $v$ and $w$(cf. \ref{sec: motivation picard iteration}). It has to be examined if this decoupled PDE has more than the solution $v=w$. If there are more solutions, it has to be analysed if those interfere during the Picard iterations. \\
As the Picard iteration is motivated by the Picard linearisation of convection terms, one can try to apply the theory developed for those linearisations.

The implemented convexification did not support the solution process. Yet, it is worth to address this approach further. Maybe it is useful to convexify intermediate solution only on fine grids as those better capture the convexity of the exact solution.

We presented methods for the \MA equation given that the right-hand side only depend on $x$ and the left-hand side only on the determinant of the Hessian. Currently DG methods are not extended to more general PDEs: There is an extension of the $C^0$ penalty method to the three-dimensional case\cite{BN2012}, but to the authors' knowledge there exist no works on other extensions. It has to be analysed, if  discretisations for right-hand sides depending on $u$ and $\nabla u$ and more complicated left-hand sides may be derived analogously. An example for one of those more complicated left-hand sides is $\mydet {D^2 u +A}$ for $A:\Omega \rightarrow \R^{d \times d}$, a PDE arising in the context of imaging optics \cite{BHP2014}.
 
But the most interesting part is the effect of the gradient penalty term introduced in Section \ref{subsec: add penalty param}:
\begin{align}
	\sum_{e \in \edgesi} \sigma_g |e |\myIntS e {\jump{ \nabla u} \jump {\nabla v}}. \tag{\ref{eq: grad penalty term}}
\end{align}
Neilan introduced it initially to retain convergence of his method for low polynomial degrees. We have seen it also improves the results of all polynomial degrees and in particular it allows to lower the polynomial degree of the Hessian ansatz space with respect to the solution's ansatz space. \\
The Picard iteration uses the Hessian of the last step its entries are always piecewise polynomials of degree $k-2$ where $k$ is the degree of the ansatz space. Thus, this penalty term is very important to gain convergent numerical schemes for the \MA equation and his impact has to be further analysed. It may be possible to improve other exisiting methods, such as the $C^0$ penalty method, by adding this additional penalty term. 
