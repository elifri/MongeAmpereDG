\section{Benchmark Examples}

Since the \MA equation became a benchmark problem for fully nonlinear second order PDEs there are some classical test problems for the two-dimensional case. All test cases are formulate for the domain $\Omega$ to be the unit square $[0,1]^2$.

\begin{test} \label{test smooth}
The first classical \MA test is the problem with the data
\[
	u=\exp( \lVert x \rVert_2^2  /2) 
	\text { and } 
	f = (1 + \lVert x \rVert_2^2) \exp( \lVert x \rVert^2).
\]
It has a very smooth solution and is even radial.

\end{test}

\begin{test}\label{test sqrt}
The data
\[
	u = - \sqrt{ 2-  \lVert x \rVert_2^2}
	\text { and } 
	f = 2\left( 2-  \lVert x \rVert_2^2 \right)^{-2}
\]
defines the second example. This test is especially interesting because the convex viscosity solution is only contained in $W^{1,p}(\Omega) $ for $p \in [0,4)$\cite{DG2006a}, i.e. it lacks $H^2$ regularity.
\end{test}

For the next two tests we define $x_0 = \left(\frac 1 2, \frac 1 2  \right)^t$.

\begin{test}\label{test singularity}
The third \MA test is also irregular. Its solution lies in $C^1$ and is given by
\[
	u=\frac 1 2 \left( \max 0 {\lVert x - x_0 \rVert_2-0.2 }  \right)^2 
	\text { and } 
	f = \max 0 {1-\frac {0.2} {\lVert x - x_0 \rVert_2} }.
\]
\end{test}


\begin{test}\label{test dirac}
The last test is determined by
\[
	u = \lVert x - x_0 \rVert_2
	\text { and } 
	f = \pi \delta_{x_0},
\]
where $\delta_{x_0}$ denotes the dirac measure with mass concentrated at $x_0$. Its solution $u$ describes a cone with origin $x_0$. This example does not have a viscosity solution, but $u$ is only an Aleksandrov solution of the problem \cite[Section 2.3.]{FO2011}.

Note that $f$ is highly non-regular. Motivated by \cite[Section 6.1.]{FO2011} we discretise $f$ by
\begin{align*}
	f_h = \begin{cases}
		\frac \pi {4h^2} & \text{ if} \singleNorm{x_1 - 0.5} \leq h \text{ and } \singleNorm{x_2 - 0.5} \leq h \\
		0	& \text{otherwise}
	\end{cases}.
\end{align*}
\end{test}


%\begin{test}\label{test rhsConst}
%A test where the analytical solution is unknown is determined by
%\[
%	u = 0 \text{ on } \partial \Omega
%	\text { and } 
%	f = 1
%\]
%defines the last example.
%\end{test}

\section{Numerical Results of a $C^0$ Penalty Method}\label{sec: numerical results brenner}

\input{load_Neilan_data.tex}

First, we analyse the performances of the algorithm, developed by Brenner et al. \cite{BGN+2011} which was introduced in Section \ref{sec: Brenner method}.
We implemented their method using the finite element tool FEniCS \cite{FEniCS}, the code is appended in \ref{app: Code Brenner}. 
To create the initial guess not a vanishing moment method, as Brenner suggested, but the solution of $\triangle u = -\sqrt{2f}$ as introduced in Section \ref{sec: initial guess} was used. 

The triangulation $\triang$ was obtained by a standard refinement as explained in Section \ref{subsec: refinement and base cells}: First the domain was split into four triangles by drawing both diagonals and afterwards each triangle was successively divided into four congruent triangles.
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\edef \n {2}
		\input{triangulation.pgf}
		\caption{Triangulation with $h=\frac 1 2$}
		\label{fig: grid1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\edef \n {4}
		\input{triangulation.pgf}
		\caption{Triangulation with $h=\frac 1 4$}
		\label{fig: grid}
	\end{subfigure}	
	\caption{Refinement of the triangulation $\triang$}
	\label{fig: grids}
\end{figure}

The arising nonlinear system is solved by PETSc configured with FEniCS default solver choice: That is a Newton based nonlinear solver that uses a line search and solves arising linear systems by a $LU$ decomposition.
%SNES Object: 1 MPI processes
%  type: newtonls
%  SNES has not been set up so information may be incomplete
% maximum iterations=10, maximum function evaluations=2000
% tolerances: relative=1e-09, absolute=1e-08, solution=1e-16
%total number of linear solver iterations=0
%total number of function evaluations=0
%SNESLineSearch Object:   1 MPI processes
% type: bt
%  interpolation: cubic
%   alpha=1.000000e-04
%  maxstep=1.000000e+08, minlambda=1.000000e-12
%   tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
%  maximum iterations=40
%KSP Object:   1 MPI processes
%type: preonly
%maximum iterations=10000, initial guess is zero
% tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
% left preconditioning
% using DEFAULT norm type for convergence test
%PC Object:   1 MPI processes
%  type: lu
%  PC has not been set up so information may be incomplete
%  LU: out-of-place factorization
%   tolerance for zero pivot 2.22045e-14
%   matrix ordering: nd
% linear system matrix = precond matrix:
% Matrix Object:     1 MPI processes
%  type: seqaij
%  rows=112, cols=112
%  total: nonzeros=2744, allocated nonzeros=2744
%  total number of mallocs used during MatSetValues calls =0
%  using I-node routines: found 80 nodes, limit used is 5
Only the absolute tolerance and the number of maximum iteration are changed from the original default values, the absolute tolerance is adjusted to $1e-8$ and the number of maximum iteration restricted to 100. 

The results can be found in figure \ref{fig: Brenner test1} and in Table \ref{tab: l2 errors test 1 Brenner} where $h$ denotes the grid width and $N$ denotes the iterations the Newton solver needed to reach the desired tolerance. 

\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
		]\MAOneBrennerTwo
    	\caption{Error for $k=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
		]\MAOneBrennerThree
 	\caption{Error for $k=3$}
	\end{subtable}
	\caption{Errors for Test \ref{test smooth}}
	\label{tab: l2 errors test 1 Brenner}
\end{table}

Fitting the data we can calculate the numerical convergence order $2.502$ for $k=2$ and $3.584$ for $k=3$ in the $L^2$ error norm as well as $2.007$ for $k=2$ and $3.028$ in the $H^1$ error norm. Thus, the results in the $H^1$ confirm the almost convergence rate $k$ as predicted by the authors (cf. also Theorem \ref{thm: error estimate brenner}).
Additionally we observe that the method also converges for polynomial degree $k=2$ with optimal rates although the prove of the error estimate do not cover the case $k<3$.

\begin{figure}[H]
\centering
	\includegraphics[scale=0.45]{plots/MA1_Brenner_l2.pdf}
	\caption{$L^2$ errors for Test \ref{test smooth}}
	\label{fig: Brenner test1}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.45]{plots/MA1_Brenner_h1.pdf}
	\caption{$H^1$ errors for Test \ref{test smooth}}
	\label{fig: Brenner test1 h1}
\end{figure}

For the second test case the Newton did not converge, not even on the coarsest grid. Also in the third test case our Newton solver had problems. You can see results of the runs for $h=1/2, \dots, 1/16$ in Table \ref{tab: l2 errors test 3 Brenner}, for finer grid the solver did not converge. Attempts to solve this example with FEniCS built in damped Newton method with different relaxation parameters did neither succeed on any grid finer than $1/16$.
\begin{table}[h]
\centering
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
		]\MAThreeBrennerTwo
    	\caption{Error for $k=2$}
    \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
				    every row 0 column 0/.style={set content=init},
				    every row 3 column 1/.style={set content=-},
				    every row 3 column 2/.style={set content=-},
				    every row 3 column 3/.style={set content=-},
				    every row 4 column 1/.style={set content=-},
				    every row 4 column 2/.style={set content=-},
				    every row 4 column 3/.style={set content=-},
		]\MAThreeBrennerThree
    	\caption{Error for $k=3$}
    \end{subtable}	\caption{Errors for Test \ref{test singularity}}
	\label{tab: l2 errors test 3 Brenner}
\end{table}

The fourth example does not have not a classical solution. As in the previous cases the method does not converge. The corresponding results are shown in Table \ref{tab: l2 errors test 4 Brenner}.

\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
					columns/l2error/.style={
				        column name=$L^2$ error,  % ... and the second
				        dec sep align,      % align on the decimal marker
				        /pgf/number format/sci e, 
				        /pgf/number format/fixed zerofill=false,  % print trailing zeros
				        /pgf/number format/sci precision=6 ,    % print 6 digits
				            },
		]\MAFourBrennerTwo
    	\caption{Error for $k=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
				    columns/l2error/.style={ /pgf/number format/sci precision=6}     % print 14 digits
		]\MAFourBrennerThree
 	\caption{Error for $k=3$}
	\end{subtable}
	\caption{Errors for Test \ref{test dirac}}
	\label{tab: l2 errors test 4 Brenner}
\end{table}

%For the last test case with constant right-hand side and zero Dirichlet boundary data Newton's method only converged on the coarsest grid such that we cannot make any statement on this test.

We see that this method behaves well for smooth solutions for which it was designed. The results show that it is inappropriate for problems with non smooth-solution or viscosity solutions.

\section{Numerical Results of a Finite Element Method based on a Disrete Hessian}\label{sec: numerical results neilan}

Neilan's algorithm from \cite{Neilan2014} introduced in Section \ref{sec: FEM discrete Hessian} was also implemented.
Additional to the numerical results on convergence that Neilan presented, it is interesting to explore what happens if we vary the polynomial degree for the Hessian ansatz space. 

The implementation was also done with the Finite Element Tool FEniCS \cite{FEniCS}, the corresponding source code can be found in \ref{app: Code Neilan}. \\
The uniform triangulation $\triang$ is the same as in the scenarios for the $C^0$ penalty method (cf. Section \ref{sec: numerical results brenner}).
Different to the initial guess suggested in \cite{Neilan2014} the solution of $\triangle u = -\sqrt{2f}$ as introduced in Section \ref{sec: initial guess} was used. All results presented base on discontinuous ansatz spaces, namely piecewise polynomial spaces as introduced in \ref{def: piecewise polySpace}.

We denote the degree of the trial space $V_h=\mathcal P_h^k$ by $k$ and the degree chosen for the Hessian ansatz space $\Sigma_h = [\mathcal{P}_h^{k_{DH}}]^{d \times d}$ by $k_{DH}$. The solver for nonlinear system of equations given by \eqref{eq: neilan eq1} and \eqref{eq: discrete hessian} or \eqref{eq: neilan eq1 + jump} and \eqref{eq: discrete hessian} is configured with the same parameters as in Section \ref{sec: numerical results brenner} if not stated otherwise. 

\input{NumericalResultsNeilan1.tex}

\input{NumericalResultsNeilan2.tex}

\input{NumericalResultsNeilan3.tex}

\input{NumericalResultsNeilan4.tex}

%\todo{symmetrised neilan and not crossed grid}

\newpage

\section{Numerical Results of DG Picard Iteration Method} \label{sec: numerical results our Method}

The Picard iteration is implemented in C++. All vector and matrix handling as well as linear solvers are provided by the Eigen library \cite{eigenweb}. To solve the quadratic program arising in the context of a convexification (cf. \eqref{eq: convex lsq}) the C++ library ipopt \cite{ipopt} was used.

The C++ implementation uses the same nested grids as have been used in the FEniCS implementations of the other DG methods (cf. Figure \ref{fig: grids}). It is obtained by dividing the unit square along the diagonals into four triangles and then refining uniformly as explained in \ref{subsec: refinement and base cells}.

\subsection{Implementation Details}

The main class of the implementation is the Tsolver class. The solver collects all information and controls the assembly and solving process.
Hence, the main method which can be found in mesh.cpp just initialises an instance of Tsolver and starts the solving process: Via a configuration file all input data is fed into the main routine. With this information the solver can initialise its members grid and Tshape, which represent the underlying grid structure and the reference cell. 
The grid implemenation is taken from the igpm\_t2\_lib and is based on the work of \cite{BMV2009}.

The main method, i.e. the algorithm from Algorithm \ref{alg: final} is implemented in Tsolver's function stepping\_MA(). It starts with an initialisation process where it reads specific problem data including
\begin{enumerate}
 \item the problem we want to solve,
 \item penalty parameters to enforce continuity and penalise the gradient jump,
 \item the start level of refinement with respect to the initial grid $start\_level$,
 \item the damping parameter $\alpha$,
 \item the number of fixed point iterations per grid $max\_its$,
 \item the maximal grid refinements $max\_levelrefinement$.
\end{enumerate}
During the initialisation the methods also updates all base cell data and leaf cell data, enumerates the degree of freedoms and computes the initial guess.\\
Afterwards the fixed point iterations begin. In every step the Generalised Poisson problem (cf. Section \ref{sec: SIPG MA}) is solved, eventually convexified (cf. Section \ref{subsec: convexification}) and afterwards combined with the solution of the last step (cf. Section \ref{subsec: add penalty param}). If the specified number of iterations has been carried out, the grid is refined, the leaf cell data updated and we start the fixed point iterations again.
We repeat this process until the maximal number of grid refinements is reached.
To illustrate this procedure, Algorithm \ref{alg: stepping} shows the function stepping\_MA() in pseudo code.
\begin{algorithm}[H]
\begin{algorithmic}
	\State Read problem specific data
%		\begin{enumerate}
%			 \item  problem\_name \Comment {to choose right-hand side and boundary conditions}
%			 \item penalty parameters   \Comment {  to enforce continuity and penalise the gradient jump}
%			 \item start\_level  \Comment { the start level of refinement with respect to the input grid}
%			 \item $\alpha$  \Comment {the damping parameter}
%			 \item maxits \Comment{ the number of fixed point iterations per grid}
%			 \item max\_levelrefinement \Comment{ the maximal refinement level }
%		\end{enumerate}
	\State Initialise base cell data
	\State Refine to $start\_level$
	\State Initialise leaf cell data
	\State Enumerate degree of freedoms
	\State (Initialise Convexifier)
	\State Initialise initial guess $u_{-1}$
	\State level $\gets start\_level$
	\State $i \gets 0$
	\While {level $\leq max\_levelrefinement$}
		\State Enumerate degree of freedoms
		\State Update base and leaf cell data
		\State (Initialise C0 converter) \Comment{only needed for convexification}
		\State cur\_it$ \gets 0$
		\While {cur\_it < $maxits$}
			\State  assemble\_MA()                              \Comment Assemble system for General Poisson problem
			\State $u_i \gets$ solution of assembled system
			\State (Convexify $u_i$)		 
			\State $u_i \gets \alpha u_{i-1}  +(1-\alpha) u_i$
			\State	cur\_it, i $\gets$ cur\_it$+1$, i$+1$
			\State Restore\_MA() 		\Comment{Store solution in leaf cells}
		\EndWhile
		\State Refine grid and update leaf cell data
		\State level $\gets$ level $+1$
	\EndWhile
\end{algorithmic}
\caption{stepping\_MA}
\label{alg: stepping}
\end{algorithm}

After we have seen the program structure a short remark of some of the arising programming issues is presented:

Before the actual system assembly, the diffusion matrix in every cell has to be updated with the modified cofactor matrix of the Hessian of the last Picard step. Note that this matrix is constant for polynomial degree $k=2$, but otherwise has to be calculated at every quadrature point. Since in general there is no connection between diffusion matrix values at the reference cell and in the leaf cell, we cannot save any memory as we did for function values and normal gradients (cf. Section \ref{subsec: ref cell} and \ref{subsec: refinement and base cells}). Hence, we store the diffusion matrix evaluated at every quadrature point in the corresponding leaf cell.
The rest of the assembly process is carried out as already mentioned in Section \ref{subsec: assembly loop}. Hereby one has to pay attention to the right scaling of quadrature weights and the right calculation of quadrature data from reference cell data as described in Examples \ref{ex: base cell trafo} and \ref{ex: leaf cell trafo}. 

If a convexification is desired, the class Convexifier takes care of the execution of the method described in Section \ref{subsec: convexification}. The linear constraints ensuring convexity on a triangle are chosen to be the ones from Theorem \ref{thm: convex cond on triangle} and the constraints for convexity across edges are given by Theorem \ref{thm: convex cond across edge}. Due to the error-proneness during the assembly of the constraint matrix $C$ the convexification is only implemented for the case $k=2$.
Note that before applying the theorems we have to convert our numerical solution from the DG formulation to a $C^0$ spline.
Hence, at first the Convexifier computes the $C^0$ spline representation of the Poisson solution using the C0converter, and then assembles the matrices $A$ and $C$ of \eqref{eq: convex lsq}. To solve this quadratic problem it uses of the open source library ipopt \cite{ipopt}.\\
Fortunately the evaluation matrix $A$ and the constraint matrix $C$ of \eqref{eq: convex lsq} depend only on the grid and thus, need only to be computed once in every refinement.

Additionally the Tsolver has a member Plotter handling all export to external files. It is able to write the solution whose coefficients are currently stored in the leaf cells to a .vtu or .dat file. Besides plotting it also administers all file streams to which information such as $L^2$ errors are written into.

\subsection{Results with Convexification}

For an initial guess the solution of $\triangle u = -\sqrt{2f}$ combined with the nested iteration approach as described in \ref{sec: initial guess} is used. The grid is chosen as in all previous implementations, i.e., a uniform refinement of a criss-crossed unit square (cf. Section \ref{sec: numerical results brenner}).
The arising linear system of equations is solved with Eigen's internal Cholesky solver.
The parameters are, if not specified differently, taken to be $\sigma=20 k^2, \sigma_G = 50, \alpha =
0.3$ and $\varepsilon = 1e-2$. We always carry out 15 steps including a convexification after every step before we refine further.
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA1_convexify.pdf}
	\caption{$L^2$ errors for Test \ref{test smooth} and additional convexification}
	\label{fig: l2 errors test smooth ourMethodConvex}
\end{figure}
The implemented convexification follows the ideas of Section \ref{subsec: convexification}. \\
The plots given in Figure \ref{fig: l2 errors test smooth ourMethodConvex} show the $L^2$ error of $u_i$ obtained by the calculations as described in Section \ref{sec: Picard Iteration Algo}. We recall that means $u_i$ is obtained by combining the old step with a convexified Poisson solution. Unfortunately, the method with the convexification process does not work as expected.
To investigate this failure a comparison of the $L^2$ error before the convexification and after is presented in Figure \ref{fig: convex before after}. As we can see the convexification process increases the error. There are several possible explanations for that:

Though the convexification algorithm produces a convex result it does not garantuee that it leaves convex input functions unchanged.

The convex function in the space of piecewise polynomials $\mathcal P^2_h$ approximating the numerical solution $u_h$ best, is not necessarily closer to the exact solution $u$ than $u_h$. %Then the convexification gives a push into the wrong direction.

An approximation of $u$ in $\mathcal P^2_h$ does not have to be convex. There exists an example where the affine Lagrange interpolant of a convex function is not convex\cite[p.3142]{AM2009}. 

To understand how \quoting{unconvex} the function was before the convexification process an examination of the vector $Cc_u$, where $C$ is the matrix containing the convexity conditions and $c_u$ are the coefficients of $L^2$ projection into $\mathcal P^2_h$ (cf. \eqref{eq: convex lsq}), is carried out: The smallest entry for a grid with $h=1/2$ is -9.77e-03, for a grid with $h=1/16$ the smallest entry is -4.26e-04. Thus, the $L^2$ projection of the exact solution does not comply with the convexity conditions.
Figure \ref{fig: convex min coeffs} shows the smallest entry of $Cc$ during the Picard iterations, where $c$ are the B\'ezier coefficients of the Poisson solutions. We see that especially for small grid widths the Poisson solutions do not fulfill the convexity conditions. 

\begin{figure}[H]
\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[scale=0.25]{plots/MA1_convexComp.pdf}
		\caption{Comparison of $L^2$ errors directly before and after convexification}
		\label{fig: convex before after}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[scale=0.25]{plots/MA1_minCoeff.pdf}
		\caption{Minimal entry in the vector $C c$}
		\label{fig: convex min coeffs}
	\end{subfigure}	
	\caption{$L^2$ errors for Test \ref{test smooth} and additional convexification}
	\label{fig: Compare test smooth ourMethodConvex}
\end{figure}


The other test cases produce similar results such that we can assume the presented convexification is not sensible in the context of the Picard iteration.

%\begin{figure}[H]
%\centering
%	\includegraphics[scale =0.4]{plots/MA2_convexify.pdf}
%	\caption{$L^2$ errors for Test \ref{test singularity} and additional convexification}
%	\label{fig: l2 errors test singularity ourMethodConvex}
%\end{figure}
%\begin{figure}[H]
%\centering
%	\includegraphics[scale =0.4]{plots/MA2_convexComp.pdf}
%	\caption{$L^2$ errors for Test \ref{test singularity} and additional convexification}
%	\label{fig: Compare test singularity ourMethodConvex}
%\end{figure}


	

\subsection{Results without Convexification}

We use the same code basis as in the previous tests but turn off every direct convexification. Additionally the basis polynomials are chosen to be the Lagrange basis polynomials. Beside that the settings are not altered, i.e., the parameter were taken to be $\sigma = 20 k^2$, $\sigma_G$=50, $\alpha=0.3$ and $\varepsilon = 1e-2$. 
In Figure \ref{fig: l2 errors test smooth ourMethod} and Table \ref{tab: l2 errors our method} the results for the the smooth Test \ref{test smooth}  could be seen: Again in the plot every point represents the $L^2$ error after a Picard step, whereas the table shows the error value after 15 iterations on the same grid.
\begin{figure}[H]
	\centering
	\includegraphics[scale =0.4]{plots/MA1.pdf}
	\caption{$L^2$ errors for Test \ref{test smooth}}
	\label{fig: l2 errors test smooth ourMethod}
\end{figure}
\begin{table}[H]
	\centering
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[
			every row 0 column 0/.style={set content=init},
		]
		{
		iterations  l2error
		0 0.116217
		1 0.0149796
		2 0.00439385
		3 0.00116137
		4 0.000413233
		5 0.0332876
		6 0.341691
		}
		\caption{$L^2$ error for $k=2$}
	\end{subtable}
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[
			every row 0 column 0/.style={set content=init},
		]
		{
			iterations  l2error
			0 0.151798
			1 0.00100398
			2 0.000145466
			3 2.2412e-05
			4 4.23972e-06
			5 0.000469984
			6 0.433164
				
		}
		\caption{$L^2$ error for $k=3$}
	\end{subtable}
	\caption{$L^2$ errors for test \ref{test smooth}}
	\label{tab: l2 errors our method}
\end{table}
 For the first four refinements the method behaves well for both $k=2$ and $k=3$. Afterwards the numerical solution departs from the actual solution.
 
 To exclude a break down of the linear solver, it was verified that the conditions of the system matrices could be handled by Eigen's cholesky solver.

%To decrease the influence of the penalty parameter $\sigma$ for quadratic polynomials strong boundary conditions were introduced, i.e., instead of forcing the boundary conditions weakly by the penalty term 
%\[
%	\sum_{e \in \edgesb} \frac \sigma {|e|} \myIntS {e} { v g}
%\]
%all coefficients of base polynomials, which have support at the boundary, were fixed such that they satisfy the boundary condition given by $g$ at a set of equidistant boundary points. 
Since the influence of the gradient penalty term (cf. Section \ref{subsec: add penalty param}) on the solution is not clear, Figure \ref{fig: sigma variation} and Table \ref{tab: sigma variation} show runs with different choices of $\sigma^G$, where $\alpha$ is still fixed to $\alpha=0.3$.
\begin{figure}[H]
	\centering
	\includegraphics[scale =0.4]{plots/MA1_deg2_sigma.pdf}
	\caption{$L^2$ errors for different choices of $\sigma^G$}
	\label{fig: sigma variation}
\end{figure}

\pgfkeys{
	/pgfplots/table/string type in dec sep align/.style={
		string type,
		postproc cell content/.code={%
			\ifnum\pgfplotstablepartno=0%
			\pgfkeys{/pgfplots/table/@cell content/.add={}{&}}
			\fi
		}%
	}
}

\begin{table}[H]
	\centering
	\pgfplotstabletypeset[
	columns={iterations, 0, 10, 30,50,100,200},
%	columns/0/.style={column name=$h$},
	columns/0/.style={column name={$\sigma^G$=0 }, dec sep align},
	columns/10/.style={column name={$\sigma^G$=10 }, dec sep align},
	columns/30/.style={column name={$\sigma^G=$30 }, dec sep align},
	columns/50/.style={column name={$\sigma^G=50$ }, dec sep align},
	columns/70/.style={column name={$\sigma^G=70$ }, dec sep align},
	columns/100/.style={column name={$\sigma^G=100$ }, dec sep align},
	columns/200/.style={column name={$\sigma^G=200$ }, dec sep align},
	every first column/.style={/pgf/number format/precision=0},
	%		every/.style={
	/pgf/number format/sci e, 
	/pgf/number format/fixed zerofill=true,  % print trailing zeros
	/pgf/number format/sci precision=4,     % print 14 digits
	/pgf/number format/precision=4     % print 14 digits
		%	}
	]{
		iterations  0 10 30 50 70 100 200
		1 0.0335905 0.0086428 0.0130318 0.0149796 0.016074 0.0170381 0.0183565
		2 0.0611877 0.00182679 0.00334291 0.00439385 0.00522693 0.00624324 0.00857697
		3 0.0837569 0.000407317 0.000845756 0.00116137 0.0014516 0.00184986 0.00297013
		4 0.185084 0.000115474 0.000309124 0.000413233 0.00050331 0.000628928 0.00101445
		5 0.276482 0.0226132 0.0328956 0.0332876 0.0322666 0.0297026 0.0207637
		6 0.351239 0.331674 0.360471 0.341691 0.360872 0.338608 0.326064
	}
	\caption{$L^2$ errors for different choices of $\sigma^G$}
	\label{tab: sigma variation}
\end{table}

As we already experienced in Section \ref{subsec: add penalty param} switching the gradient penalty off, i.e., $\sigma^G=0$, proves useless. However, decreasing the penalty parameter yields better approximation results, yet varying $\sigma^G$ does not change the instability of the method.

Furthermore, to test the choice of the parameter $\alpha$, Test \ref{test smooth} was carried out for different choices of $\alpha$, the penalty parameter $\sigma^G$ was taken to be 10. The results can be found in Figure \ref{fig: alpha variation} and Table \ref{tab: alpha variation}. %, Figure \ref{fig: alpha close up} shows a Close up of the first iterations.
 
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA1_deg2_alpha.pdf}
	\caption{$L^2$ errors for different choices of $\alpha$}
	\label{fig: alpha variation}
\end{figure}
%
%\begin{figure}[H]
%\centering
%	\includegraphics[scale =0.35]{plots/MA1_deg2_alphaCloseUp.pdf}
%	\caption{$L^2$ errors for different choices of $\alpha$}
%	\label{fig: alpha close up}
%\end{figure}

\begin{table}[H]
	\centering
	\pgfplotstabletypeset[
		columns/0/.style={column name=$h$},
		columns/1/.style={column name={$\alpha$=0.1 }, dec sep align,      % align on the decimal marker
			},
		columns/2/.style={column name={$\alpha=0.3$ }, dec sep align},
		columns/3/.style={column name={$\alpha=0.5$ }, dec sep align},
		columns/4/.style={column name={$\alpha=0.7$ }, dec sep align},
		every first column/.style={string type},
%		every/.style={
		/pgf/number format/sci e, 
%		/pgf/number format/fixed zerofill=true,  % print trailing zeros
		/pgf/number format/sci precision=4 ,    % print 14 digits
		/pgf/number format/precision=4     % print 14 digits
		%	}
	]{
		{$h$} 1 2 3 4
		{1/2} 0.00999324  0.00864529 0.00864962 0.00864956
		{1/4} 0.00260673 0.00182038 0.00180498 0.00180486
		{1/8} 0.000986361 0.000407317 0.000395512 0.000395319
		{1/16} 0.000363884 0.000115474 0.000154169 0.213579
		{1/32} 0.000144468 0.0226132 0.30321 0.331661
		{1/64} 0.000967598 0.331674 0.361392 0.320555

	}
	\caption{$L^2$ errors for different choices of $\alpha$}
	\label{tab: alpha variation}
\end{table}

As expected a larger choice of $\alpha$ leads to a faster decrease of the error, on the coarsest grids all solutions Picard iterations converge to approximations with the same error norm. One can verify that those approximations are numerically equal. We observe that for $\alpha=0.1$ the choice of 15 iterations is too small as we obtain a larger $L^2$ error than with the other values.\\
But as in the previous setting at some point all methods become instable and diverge. Though we note that choosing a smaller $\alpha$ slows down this process. 

  \begin{figure}[H]
  	\centering
  	\includegraphics[scale =0.37]{plots/MA3.pdf}
  	\caption{$L^2$ errors for Test \ref{test sqrt}}
  	\label{fig: l2 errors test sqrt ourMethod}
  \end{figure}
  
  
  \begin{figure}[H]
  	\centering
  	\includegraphics[scale =0.37]{plots/MA2.pdf}
  	\caption{$L^2$ errors for Test \ref{test singularity}}
  	\label{fig: l2 errors test singularity ourMethod}
  \end{figure}
We get similiar results in the next two cases albeit the fixed point iterations performs worse in the second case which lacks $H^2$ regularity. The $L^2$ errors are shown in the Figures \ref{fig: l2 errors test sqrt ourMethod} and \ref{fig: l2 errors test singularity ourMethod}, $\alpha$ was taken to be 0.3 and $\sigma^G$ to be 10. In the last example \ref{test dirac} the Picard iteration showed no signs of convergence at all.

The results are validated by a reference implementation using again the finite element tool FEniCS. Although it was not possible to include the modified cofactor matrix to the bilinear form, the error behaves as in the computational results of the C++ code. It decreases for the first four refinements and diverges afterwards. 

Additionally in the FEniCS variant one can replace the cofactor matrix of the Hessian, which is evaluated piecewise, by the discrete Hessian as defined in Section \ref{def: discrete Hessian} for a symmetric ansatz space $\Sigma_h$. The code can be found in \ref{app: our code}. Experiments with this symmetric discrete Hessian also end in an unstable method.
