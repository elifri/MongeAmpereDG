\section{Benchmark Examples}

Since the \MA equation became a benchmark problem for fully nonlinear second order PDEs there are same classical test problems for the two-dimensional case. All test cases are solved on the unitsquare $\Omega=[0,1]^2$.

\begin{test} \label{test smooth}
The first classical \MA test is the problem with the data
\[
	u=\exp( \lVert x \rVert_2^2  /2) 
	\text { and } 
	f = (1 + \lVert x \rVert_2^2) \exp( \lVert x \rVert^2).
\]
It has a very smooth solution and is even radial.

\end{test}

\begin{test}\label{test sqrt}
The data
\[
	u = - \sqrt{ 2-  \lVert x \rVert_2^2}
	\text { and } 
	f = 2\left( 2-  \lVert x \rVert_2^2 \right)^{-2}
\]
defines the second example. This test is especially interesting because the convex viscosity solution is only contained in $W^{1,p}(\Omega) $ for $p \in [0,4)$\cite{DG2006a}, i.e. it lacks $H^2$ regularity.
\end{test}

For the next two tests we define $x_0 = \left(\frac 1 2, \frac 1 2  \right)^t$.

\begin{test}\label{test singularity}
The third \MA test is yet more regular. Its solution lies in $C^1$ and it is given by
\[
	u=\frac 1 2 \left( \max 0 {\lVert x - x_0 \rVert_2-0.2 }  \right)^2 
	\text { and } 
	f = \max 0 {1-\frac {0.2} {\lVert x - x_0 \rVert_2} }.
\]
\end{test}


\begin{test}\label{test dirac}
A test determined by
\[
	u = \lVert x - x_0 \rVert_2
	\text { and } 
	f = \pi \delta_{x_0}
\]
describes a cone with origin $x_0$. Note, $f$ is highly non regular. This example does not have a viscosity solution, but $u$ is only an Aleksandrov solution of the problem \cite[Section 2.3.]{FO2011}.
\end{test}


\begin{test}\label{test rhsConst}
A test where the analytical solution is unknown is determined by
\[
	u = 0 \text{ on } \partial \Omega
	\text { and } 
	f = 1
\]
defines the last example.
\end{test}

\section{Numerical Results of a $C^0$ Penalty Method}

\input{load_Neilan_data.tex}

For reference I implemented the algorithm introduced in Section \ref{sec: Brenner method}.
We implemented their presented method using the finite element tool FEniCS \cite{FEniCS}, the Code is append in Appendix. 
To create our initial guess I did not use a vanishing moment method as Brenner suggested, but the solution of $\triangle u = -\sqrt{2f}$ as introduced in \ref{sec: initial guess}. 

I solved the arising nonlinear system of equations with PETSc' basic line search Newton method. 
The results can be found in figure \ref{fig: Brenner test1} and in table \ref{tab: l2 errors test 1 Brenner} where $h$ denotes the grid width and $N$ denotes the iterations needed to reach the desired tolerance. 

\begin{table}[h]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%				    every row 0 column 0/.style={set content=init},
%		]\MAOneBrennerTwo
    	\caption{Error for $k=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%				    every row 0 column 0/.style={set content=init},
%		]\MAOneBrennerThree
 	\caption{Error for $k=3$}
	\end{subtable}
	\caption{Errors for test case \ref{test smooth}}
	\label{tab: l2 errors test 1 Brenner}
\end{table}
\begin{figure}[h!]
\centering
	\includegraphics[scale=0.35]{../../FEniCS/diagrams/MA1_Brenner_l2.pdf}
	\caption{$L^2$ errors for test case \ref{test smooth}}
	\label{fig: Brenner test1}
\end{figure}
Fitting the data we can calculate then convergence order  $2.576$ for $k=2$ and $3.306$ for $k=3$.

For the second test case the Newton did not even on the coarsest grid converge. Also in the third test case our Newton solver had problems. You can see results of the runs for $h=1/2, \dots, 1/16$ in table \ref{tab: l2 errors test 3 Brenner}, for finer grid the solver did not converge. My attempts to solve this example with FEniCS' built in damped Newton method with different relaxation parameters did also not succeed on any grid finer than $1/16$.
\begin{table}[h]
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%				    every row 0 column 0/.style={set content=init},
%		]\MAThreeBrennerTwo
    	\caption{Error for $k=2$}
	\caption{Errors for test case \ref{test singularity}}
	\label{tab: l2 errors test 3 Brenner}
\end{table}

The fourth example does not have not a classical solution. As in the previous cases the method does not converge. The corresponding results are shown in table \ref{tab: l2 errors test 4 Brenner}.

\begin{table}[h]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%				    every row 0 column 0/.style={set content=init},
%				    columns/l2error/.style={ /pgf/number format/sci precision=6}     % print 14 digits
%		]\MAFourBrennerTwo
    	\caption{Error for $k=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%				    every row 0 column 0/.style={set content=init},
%				    columns/l2error/.style={ /pgf/number format/sci precision=6}     % print 14 digits
%		]\MAFourBrennerThree
 	\caption{Error for $k=3$}
	\end{subtable}
	\caption{Errors for test case \ref{test dirac}}
	\label{tab: l2 errors test 4 Brenner}
\end{table}

For the last test case with constant right-hand side and zero Dirichlet boundary data Newton's method only converged on the coarsest grid such that we cannot make any statement on this test.

We see that this method behaves well for smooth solutions. This method was designed to detect classical solutions and indeed to solve problems with non smooth solution and viscosity solutions the method is inappropriate.

\newpage
\section{Numerical Results of a Finite Element Method based on a Disrete Hessian}

I also implemented the algorithm introduced in Section \ref{sec: FEM discrete Hessian}.
Additional to the numerical results Neilan presented it is interesting to explore what happens if we vary the polynomial degree for the Hessian ansatz space. 

The implementation was also done with the Finite Element Tool FEniCS \cite{FEniCS}, the corresponding source code can be found in the appendix \ref{app: Code Neilan}. \\
The uniform triangulation $\triang$ was obtained as also explained in \ref{subsec: refinement and base cells}: First the domain was split into four triangles by drawing both diagonals and afterwards each triangle was successively divided into four congruent triangles. \\
Different to the initial guess suggested in \cite{Neilan2014} I used the solution of $\triangle u = -\sqrt{2f}$ as introduced in \ref{sec: initial guess}. All results presented base on discontinuous ansatz spaces, namely piecewise polynomial spaces as introduced in \ref{def: piecewise polySpace}.
We denote the degree of the trial space $V_h=\mathcal P_h^k$ by $k$ and the degree chosen for the Hessian ansatz space $\Sigma_h = [\mathcal{P}_h^{k_{DH}}]^{d \times d}$ by $k_{DH}$. The nonlinear system of equations given by \eqref{eq: neilan eq1} and \eqref{eq: discrete hessian} or \eqref{eq: neilan eq1 + jump} and \eqref{eq: discrete hessian}, respectively was solved with PETSc' basic line search Newton method. 
%SNES Object: 1 MPI processes
%  type: newtonls
%  SNES has not been set up so information may be incomplete
 % maximum iterations=10, maximum function evaluations=2000
 % tolerances: relative=1e-09, absolute=1e-08, solution=1e-16
  %total number of linear solver iterations=0
  %total number of function evaluations=0
  %SNESLineSearch Object:   1 MPI processes
   % type: bt
    %  interpolation: cubic
   %   alpha=1.000000e-04
  %  maxstep=1.000000e+08, minlambda=1.000000e-12
 %   tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
  %  maximum iterations=40
  %KSP Object:   1 MPI processes
    %type: preonly
    %maximum iterations=10000, initial guess is zero
   % tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
   % left preconditioning
   % using DEFAULT norm type for convergence test
  %PC Object:   1 MPI processes
  %  type: lu
  %  PC has not been set up so information may be incomplete
    %  LU: out-of-place factorization
   %   tolerance for zero pivot 2.22045e-14
   %   matrix ordering: nd
   % linear system matrix = precond matrix:
   % Matrix Object:     1 MPI processes
    %  type: seqaij
    %  rows=112, cols=112
    %  total: nonzeros=2744, allocated nonzeros=2744
    %  total number of mallocs used during MatSetValues calls =0
      %  using I-node routines: found 80 nodes, limit used is 5

Most parameters were set to the default PETSc values %which includes the solver uses cubic backtracking 
but the absolute tolerance was adjusted to $1e-8$ and the number of maximum iteration was restricted to 100. 

Figure \ref{fig: l2 errors test 1} shows the $L^2$ error of all performances of the first test case that have converged, the polynomial degrees $k$ were taken to be $1,\dots,3$ and $k_{DH}$ equal to all variants $0, \dots, k$. In Figure \ref{fig: h1 errors test 1} the corresponding $H^1$ errors are plotted.
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA1_Neilan_l2.pdf}
	\caption{$L^2$ errors for test case \ref{test smooth}}
	\label{fig: l2 errors test 1}
\end{figure}
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA1_Neilan_h1.pdf}
	\caption{$H^1$ errors for test case \ref{test smooth}}
	\label{fig: h1 errors test 1}
\end{figure}

Due to the large number of degree of freedoms the I could not obtain any results for the cases $k=2,3$ and $k_{DH}=2,3$ on the finest grid, for $k=3$ and $k_{DH}=3$ even on a mesh with $h=1/64$ the requested memory was too much.

As also experienced by Neilan the method does not work for the polynomial degree $k=1$. Similarly, in runs with a low polynomial degree $k_{DH}$ Newton's method did not converge. \todo{LU hat nicht funktioniert}
The results for the runs with $k=3$ are shown more detailed in Table \ref{tab: l2 errors test 1 deg 2}, in both tables the column $N$ refers to the number of iterations the Newton solver needed to reach the desired tolerance. 
\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
		]\MAOnedegThreeThree
    	\caption{Error for $k=3, k_{DH}=3$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
				    every row 0 column 0/.style={set content=init},
		]\MAOnedegThreeTwo
 	\caption{Error for $k=3, k_{DH}=2$}
	\end{subtable}
	\caption{Errors for test case \ref{test smooth}}
	\label{tab: l2 errors test 1 deg 2}
\end{table}
Considering this results in the smooth test scenario we observe that the error almost do not alter for different polynomial degrees $k_{DH}$ if both converge. Albeit for fine grids the methods with less difference between the two degrees seem to perform better. The calculated numerical orders as shown in table \ref{tab: order} supports this first intuition.

\begin{table}[H]
\begin{subtable}[b]{0.45\textwidth}
\centering
	\pgfplotstabletypeset
	{
		k $k_{DH}$ {numerical order}
		2 1  1.50338
		2 2  2.24427
		3 2 2.63597
		3 3 3.64758
	}
	\caption{numerical order in $L2$ norm}
\end{subtable}
\begin{subtable}[b]{0.45\textwidth}
	\pgfplotstabletypeset
	{
		k $k_{DH}$ {numerical order}
		2 1  1.65382 
		2 2  1.973
		3 2 2.39616
		3 3 2.97876
	}
	\caption{numerical order in $H1$ norm}
	\end{subtable}
\caption{numerical order in test \ref{test smooth}}
\label{tab: order}
\end{table}

This test scenario was also performed with the additional normal jump penalty term as stated in \eqref{eq: neilan eq1 + jump} weighted with $\eta$ equal to 50 leading to the results shown in figure \ref{fig: l2 errors test 1 jump} and the tables \ref{tab: l2 errors test 1 deg 2 jump}.

\begin{figure}[h!]
\centering
	\includegraphics[scale=0.4]{../../FEniCS/diagrams/MA1_Neilan_GradJump_l2.pdf}
	\caption{$L^2$ errors for test case \ref{test smooth} and additional gradient jump penalty}
	\label{fig: l2 errors test 1 jump}
\end{figure}

\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA1_Neilan_GradJump_h1.pdf}
	\caption{$H^1$ errors for test case \ref{test smooth} and additional gradient jump penalty}
	\label{fig: h1 errors test 1 jump}
\end{figure}
\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[
		columns={iterations, l2error, h1error,N},
		    every row 0 column 0/.style={set content=init},
		    every row 7 column 1/.style={set content={-}},
		    every row 7 column 2/.style={set content={-}},
		    every row 7 column 3/.style={set content={-}},
		]\MAOneJumpdegTwoTwo
    	\caption{Error for $k=2, k_{DH}=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
		    every row 0 column 0/.style={set content=init},
		]\MAOneJumpdegTwoZero
	\caption{Error for $k=2, k_{DH}=0$}
	\end{subtable}
	\caption{Errors for test case \ref{test smooth}}
	\label{tab: l2 errors test 1 deg 2 jump}
\end{table}
As claimed by Neilan the additional penalisation leads to a convergend method even for low polynomial degrees, even in the cases where only $k_{DH}$ was taken to be small. The calculated error norms indicate more clearly than in the non penalised runs that variation of $k_{DH}$ yields only to a slight loss of accuracy. The corresponding numerical orders can be found in \ref{tab: order jump}.
\begin{table}[H]
\centering
\begin{subtable}[b]{0.45\textwidth}
	\pgfplotstabletypeset
	{
		k $k_{DH}$ {numerical order}
		1 0 1.32061
		2 0 2.69871
		2 1 2.93488
		2 2 3.0406
		3 1 3.63116
		3 2 4.01895
		3 3 4.0588
	}
	\caption{numerical order in $L2$ norm}
	\end{subtable}
	\begin{subtable}[b]{0.45\textwidth}
	\pgfplotstabletypeset
	{
		k $k_{DH}$ {numerical order}
		1 0 0.978853
		2 0 2.24561
		2 1 2.24836
		2 2 2.28606
		3 1 3.03434
		3 2 3.0359
		3 3  3.0343
	}
	\caption{numerical order in $H1$ norm}
	\end{subtable}
	\caption{numerical order with jump penalty in test \ref{test smooth}}
\label{tab: order jump}
\end{table}


The second test two was carried out for $\eta=20k^2$. Unfortunalety the Newton solver did not converge after the mesh was once refined. Table \ref{tab: l2 errors test 2} show the results for this one step, it shows only two degree pairs for other degree paris behaved similarly.

%\begin{figure}[H]
%\centering
%	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA2_Neilan_l2.pdf}
%	\caption{$L^2$ errors for test case \ref{test sqrt} }
%	\label{fig: l2 errors test 2}
%\end{figure}

\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[
%		columns={iterations, l2error, h1error,N},
%		    every row 0 column 0/.style={set content=init},
%		]{\MATwodegTwoTwo}
    	\caption{Error for $k=2, k_{DH}=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%		    every row 0 column 0/.style={set content=init},
%		]{\MATwodegThreeThree}
	\caption{Error for $k=3, k_{DH}=3$}
	\end{subtable}
	\caption{Errors for test case \ref{test sqrt}}
	\label{tab: l2 errors test 2}
\end{table}

Switching the penalisation of gradient jumps on the method performs again well, its results are shown in figure \ref{fig: l2 errors test 2 jump}. 

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
\centering
	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA2_Neilan_GradJump_l2.pdf}
\end{subfigure}

\begin{subfigure}{\textwidth}
\centering
	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA2_Neilan_GradJump_h1.pdf}
\end{subfigure}
	\caption{$L^2$ and $H^1$ errors for test case \ref{test sqrt} and additional gradient jump penalty}
	\label{fig: l2 errors test 2 jump}
\end{figure}

Similar are results for the third case, but now also an additional penalisation of the jump of the gradient across edges do not yield to better results. All converged iteration steps are given in the tables \ref{tab: l2 errors test 3} and \ref{tab: l2 errors test 3 jump}. 

\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[
%		columns={iterations, l2error, h1error,N},
%		    every row 0 column 0/.style={set content=init},
%		]{\MAThreedegTwoTwo}
    	\caption{Error for $k=2, k_{DH}=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
%		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
%		    every row 0 column 0/.style={set content=init},
%		]{\MAThreedegThreeThree}
	\caption{Error for $k=3, k_{DH}=3$}
	\end{subtable}
	\caption{Errors for test case \ref{test singularity}}
	\label{tab: l2 errors test 3}
\end{table}


\begin{table}[H]
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[
		columns={iterations, l2error, h1error,N},
		    every row 0 column 0/.style={set content=init},
		]{\MAThreeJumpdegTwoTwo}
    	\caption{Error for $k=2, k_{DH}=2$}
   \end{subtable}
   ~
	\begin{subtable}[b]{0.45\textwidth}
		\centering
		\pgfplotstabletypeset[columns={iterations, l2error, h1error,N},
		    every row 0 column 0/.style={set content=init},
		]{\MAThreeJumpdegThreeThree}
	\caption{Error for $k=3, k_{DH}=3$}
	\end{subtable}
	\caption{Errors for test case \ref{test singularity} and additional gradient jump penalty}
	\label{tab: l2 errors test 3 jump}
\end{table}


%\begin{figure}[H]
%\centering
%\begin{subfigure}{\textwidth}
%	\includegraphics[scale =0.4]{../../FEniCS/diagrams/MA3_Neilan_GradJump_l2.pdf}
%\end{subfigure}
%	\caption{$L^2$ and $H^1$ errors for test case \ref{test sqrt} and additional gradient jump penalty}
%	\label{fig: l2 errors test 3 jump}
%\end{figure}


\newpage

\section{Numerical Results of Our DG Method}

Our method was implemented in pure C++. The underlying grid structure was taken from the implementation of \cite{BMV2009}, handling vectors and matrices as well as was linear solver were provided by the Eigen library \cite{eigenweb}. To solve the quadratic program arising in the context of convexification the C++ library ipopt \cite{ipopt} was used.

\todo{mesh}

\subsection{Implementation Details}

The main class of the implementation is the Tsolver class. Here come all information together and the solver controls the assembly and solving process. 
We make the clear looking at a . The main method can be found in mesh.cpp. At the beginning all input data is fed into the program. With this information the solver can initialise the grid and the reference cell. 
The grid is taken from the igpm\_t2\_lib and based on the work of \cite{BMV2009} and all reference data is handled in the class Tshape.

\subsubsection{alternative 1}
The main work is done in Tsolver's function stepping\_MA(). It starts with an initialisation process where it reads specific problem data which includes
\begin{enumerate}
 \item  the problem we want to solve,
 \item penalty parameters to enforce continuity and penalise the gradient jump,
 \item the start level of refinement with respect to the initial grid,
 \item the damping paramter alpha,
 \item the number of fixed point iterations per grid,
 \item the maximal grid refinements.
\end{enumerate}
During the initialisation it also updates all base cell data, the leaf cell data, enumerate the degree of freedoms and the initial guess.
Afterwards the fixed point iterations begin, in every step the General Poisson Problem is solved, eventually convexified and afterwards with the last solution step combined. If the specified number of iterations has been carried out, the grid is refined, the leaf cell data updated and we start the fixed point iterations again.
We repeat this process until the maximal number of grid refinements is reached.


\subsubsection{alternative 2}
The main work is done in Tsolver's function stepping\_MA(). Algorithm \ref{alg: stepping} illustrates how this function works.
\begin{algorithm}[H]
\begin{algorithmic}
	\State Read problem specific data:
		\begin{enumerate}
			 \item  problem\_name \Comment {to choose right-hand side and boundary conditions}
			 \item penalty parameters   \Comment {  to enforce continuity and penalise the gradient jump}
			 \item start\_level  \Comment { the start level of refinement with respect to the input grid}
			 \item $\alpha$  \Comment {the damping paramter}
			 \item maxits \Comment{ the number of fixed point iterations per grid}
			 \item max\_levelrefinement \Comment{ the maximal refinement level }
		\end{enumerate}
	\State Initialise base cell data
	\State Refine to start\_level
	\State Initialise leaf cell data
	\State Enumerate degree of freedoms
	\State (Initialise Convexifier)
	\State Initialise initial guess $u_{-1}$
	\State $i \gets 0$
	\While {level < max\_levelrefinement}
		\State Enumerate degree of freedoms
		\State (Initialise C0 converter) \Comment{only needed for convexification}
		\State cur\_it$ \gets 0$
		\While {cur\_it < maxits}
			\State  assemble\_MA()                              \Comment Assemble system for General Poisson Problem
			\State $u_i \gets$ solution of assembled system
			\State (Convexify $u_i$)		 
			\State $u_i \gets \alpha u_{i-1}  +(1-\alpha) u_i$
			\State	cur\_it, i $\gets$ cur\_it$+1$, i$+1$
			\State Restore\_MA() 		\Comment{Store solution in leaf cells}
		\EndWhile
		\State Refine grid and update leaf cell data
		\State level $\gets$ level $+1$
	\EndWhile
\end{algorithmic}
\caption{stepping\_MA}
\label{alg: stepping}
\end{algorithm}

Direct at the beginning of system assembly the diffusion matrix in every cell has to be updated with the current cofactor matrix of the Hessian. Note, that this matrix is constant for polynomial degree $k=2$ and otherwise has to be calculated for every quadrature point. Since in general here is no connection between diffusion matrix values at the reference cell and in the leaf cell, the values of the diffusion matrix are stored in the leaf cells.
The assembly process is carried out as already mentioned in algorithm \ref{alg: assembling}. Hereby one has to pay attention to the right scaling of quadrature weights and the right calculation of quadrature data from reference cell data as described in Examples \ref{ex: base cell trafo} and \ref{ex: leaf cell trafo}. 

If a convexification is desired the class Convexifier takes care of the assembly of the matrices $A$ and $C$ for \eqref{eq: convex lsq} and deals with ipopt's interface to solve the quadratic program.

Additionally the Tsolver has a member Plotter handling all export to files. It is able to write the solution which's coefficients are currently stored in the leaf cells to a .vtu file or .dat file. Besides plotting it also administers all file stream to which information such as $L2$ errors are written into.

\subsection{Results with Convexification}

For a initial guess I also used the solution of $\triangle u = -\sqrt{2f}$ combined with the multilevel approach as described in \ref{sec: initial guess}.
The arising linear system of equations was solved with Eigen's internal Cholesky solver.
The parameters were if not other specified taken to be $\sigma=10 k^2, \sigma_G = 50, \alpha =
0.3$ and $\varepsilon = 1e-2$. We always carry out 15 steps including a convexification after every step before we refine further. The implemented convexification followed the ideas of Section \ref{subsec: convexification}. 

The linear constraints ensuring convexity on a triangle are chosen the one from theorem \ref{thm: convex cond on triangle} and the constrains for convexity across edges were given by theorem \ref{thm: convex cond across edge}. Due to the error-proneness during the assembly of $C$ the convexification was only implement for the case $k=2$. Note, that before applying the theorems of section \ref{subsec: convexification} we have to convert our numerical solution from the DG formulation to a $C^0$ spline.

The linearly constrained least square problem \eqref{eq: convex lsq}  was reformulated as a quadratic program with linear constraints using normal equations. 
The quadratic program was solved with the C++ library ipopt \cite{ipopt}. Fortunately the evaluation matrix $A$, and the constraint matrix $C$ of \eqref{eq: convex lsq} only depend on the grid. Hence they need only to be assembled once after every refinement.


The results shown in figure \ref{fig: l2 errors test smooth ourMethodConvex} are very disillusioning.
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA1_convexify.pdf}
	\caption{$L^2$ errors for test case \ref{test smooth} and additional convexification}
	\label{fig: l2 errors test smooth ourMethodConvex}
\end{figure}
We plotted in figure \ref{fig: convex before after} the $L2$ errors before the convexification step and after it. As we can see the convexification process increases the error. There are several possible explanations for that:\\
Though the used convexification algorithm produces a convex result it does not garantuee that it leaves convex input functions unchanged.\\
The convex function in $\mathcal P^2_h$ approximating the numerical solution $u_h$ best is not necessarily closer to the exact solution $u$ than $u_h$. Then the convexification gives a push into the wrong direction. \\
Also the best approximation of $u$ into $\mathcal P^2_h$ does not have to be convex\todo{aguilera}. I have examined for the triangulation with $h=1/2$ the vector $Cc_u$ where $c_u$ are the coefficients of $L2$ projection into $\mathcal P^2_h$ and its minimal entry is -9.77e-03, for a grid with $h=1/16$ the minimal entry is -4.26e-04. Thus, the $L2$ projection of the exact solution does not comply with the convexity conditions.
Figure \ref{fig: convex min coeffs} plots the minimal entry of $Cc$ for every Generalised Poisson solution vector $c$. We see that especially for small grid widths the solutions do not fulfill the convexity conditions. \todo{grid structure maxima etc.}


\begin{figure}[H]
\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[scale=0.25]{plots/MA1_convexComp.pdf}
		\caption{Comparison of $L2$ errors directly before and after convexification}
		\label{fig: convex before after}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[scale=0.25]{plots/MA1_minCoeff.pdf}
		\caption{Comparison of $L2$ errors directly before and after convexification}
		\label{fig: convex min coeffs}
	\end{subfigure}	
	\caption{$L^2$ errors for test case \ref{test smooth} and additional convexification}
	\label{fig: Compare test smooth ourMethodConvex}
\end{figure}


The other test cases produce similar results as can be seen in figure \ref{fig: l2 errors test singularity ourMethodConvex} ... \todo{}

\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA2_convexify.pdf}
	\caption{$L^2$ errors for test case \ref{test singularity} and additional convexification}
	\label{fig: l2 errors test singularity ourMethodConvex}
\end{figure}
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA2_convexComp.pdf}
	\caption{$L^2$ errors for test case \ref{test singularity} and additional convexification}
	\label{fig: Compare test singularity ourMethodConvex}
\end{figure}



\subsection{Results without Convexification}

We used the same code basis as in the previous tests but turned off every direct convexification.
In the first figure \ref{fig: l2 errors test smooth ourMethod} we can see the results for the the smooth test case \ref{test smooth}.
\begin{figure}[h!]
	\centering
	\includegraphics[scale =0.4]{plots/MA1.pdf}
	\caption{$L^2$ errors for test case \ref{test smooth}}
	\label{fig: l2 errors test smooth ourMethod}
\end{figure}
 For the first four refinements the method behaves well for both $k=2$ and $k=3$. Afterwards the numerical solution departs from the actual solution although the $L^2$ error seems to not excel $1$.
 
 We get similiar results in the next two cases albeit the fixed point iterations performs worse in the second case which lacks $H^2$ regularity. The $L2$ errors are shown in the figures \ref{fig: l2 errors test sqrt ourMethod} and \ref{fig: l2 errors test singularity ourMethod}.
 
\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA3.pdf}
	\caption{$L^2$ errors for test case \ref{test sqrt}}
	\label{fig: l2 errors test sqrt ourMethod}
\end{figure}


\begin{figure}[H]
\centering
	\includegraphics[scale =0.4]{plots/MA2.pdf}
	\caption{$L^2$ errors for test case \ref{test singularity}}
	\label{fig: l2 errors test singularity ourMethod}
\end{figure}

The results were validated by a reference implementation using the finite element tool FEniCS. Albeit I was not able to include the modified cofactor matrix to the bilinear form the error behaved as in the computational results of the C++ code. It decreased for the first four refinements and diverged afterwards. 

Additionally one can in the FEniCS variant the cofactor matrix of the Hessian which is evaluated piecewise replace by the discrete Hessian as defined in \ref{def: discrete Hessian} for a symmetric ansatz space $\Sigma_h$. The code can be found in the Appendix \todo{ref}.