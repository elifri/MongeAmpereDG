In this chapter we develop a \emph{Picard iteration} aiming to solve the \MA equation. To the author's knowledge this is the first try to apply the \emph{Picard linearisation} on the \MA equation. 
We first motivate and derive the Picard iteration and its discretisation.
Afterwards we compare the method with the classical Netwon's method and the $C^0$ penalty method presented in the previous chapter (cf. Section \ref{sec: Brenner method}). We try to transfer results on the penalty method to the newly derived Picard iteration method. 
In the last section first numerical results are presented. Since the Picard iteration does not perform well on a first example we discuss improvements of the newly derived method.


\section{A Picard Iteration for the \MA equation} \label{sec: motivation picard iteration}
For quasilinear PDEs (cf. Definition \ref{def: categories of PDEs}) such as the convection equation $\nabla \cdot (A(u) \nabla u ) = f$ it is common to determine the solution via a fixed point iteration \cite{Deblois1997,LL1995,MNK2009}. The key aspect is a decoupling of the coefficient matrix $A(u)$ and $\nabla u$. Hence, one solves iteratively for $i\in \N$ the equations
\[
	\nabla \cdot (A(u^{i} )\nabla u^{i+1}) = f. %  \quad \text{      and      }\quad \nabla \cdot (A(u^{i+1}) \nabla u^{i}) = f, \text{ respectively}.
\] 
This kind of linearisation is also called \emph{Picard linearisation} method and is very favoured to linearise the convection term in the Navier-Stokes equations.

In this spirit we aim to decouple of the derivatives in the strongly nonlinear \MA such that the decoupled PDEs are linear. 

Before we start, we slightly change notation: Differing to the preceding sections we denote the elements of $\Omega \subset \R^2$ by  $(x,y)^t$. To shorten and facilitate terms we refer to their partial derivatives with subscripts as for example $\dxy{x}{y} u = u_{xy}$.
Thus, the two-dimensional \MA equation \eqref{eq: MA eq} states
\begin{align}
 \mydet{D^2 u} &= f \nonumber \\
 	\Leftrightarrow \qquad  \dyy u{x}  \dyy u{y}  -\dyx u {x}{y} \dyx u {y}{x} &= f. \label{eq:mongeAmpere detForm}
\end{align}

First, we multiply \eqref{eq:mongeAmpere detForm} by $2$ and expand the left-hand side
\begin{align}
% 	2\dyy u {x} \dyy u {y}-2\dyx u {x}{y} \dyx u{y}{x}  &= 2 f \\
 	\dyy u {x} \dyy u {y}+\dyy u {x} \dyy u {y} -\dyx u {x}{y} \dyx u{y}{x} -\dyx u {x}{y} \dyx u{y}{x} &=2 f. 
\end{align}

Formally decoupling into the two functions $v = u ,w = u$ and reordering terms we have
\begin{align}
	w_{yy} v_{xx}- w_{xy} v_{yx} - w_{yx} v_{xy} +w_{xx} v_{yy} = 2f. \label{eq:decoupled PDE start}
\end{align}
Rewriting this by matrix Frobenius product (cf. Definition \ref{def: frobenius product}) leads to
\begin{align}
 \begin{pmatrix} w_{yy} & -w_{xy}  \\ -w_{yx} & w_{xx} \end{pmatrix}: \begin{pmatrix} v_{xx} & v_{yx}  \\  v_{xy} & v_{yy} \end{pmatrix} = 2f.
\end{align}
We observe that the left matrix is the cofactor matrix of the Hessian (cf. Definition \ref{def: cof matrix}) and the right one is the Hessian itself and hence
\begin{align}
		\mycof {D^2 w }:D^2 v  = 2f. \label{eq:short formula}
\end{align}
Note that this derivation of decoupling of $v$ and $w$ is chosen for it is very intuitive. Alternatively \eqref{eq:short formula} can be derived directly from the identity $\mycof {D^2 u }:D^2 u  = d \mydet{D^2 u}$ stated in Lemma \ref{la: rel det cofactor}.

Substituting the identity of Lemma \ref{la: An application of the divergernce product rule} and multiplying with -1 we find
\begin{align}
	- \nabla \cdot \left( \mycof{ D^2 w} \nabla v \right)  = -2f.  \label{eq:decoupled PDE}
\end{align}

For a fixed $w$ the left-hand side now is linear in $v$ opposed to the strong nonlinearity in $u$ in the original PDE. It is a Generalised Poisson equation in $v$ as we have defined in Definition \ref{def: General Poisson Problem}. 
Furthermore as it is easy to see in \eqref{eq:decoupled PDE start} the left-hand side is symmetric in $v$ and $w$. Thus, it is natural to examine a sequence of PDEs where alternating either $v$ or $w$ is fixed. 

Adding the Dirichlet boundary condition we obtain the Picard iteration
\begin{align}
	\begin{split}
	- \nabla \cdot \left( \mycof{ D^2 u^i} \nabla u^{i+1} \right)  &= -2f  \text{ in } \Omega \\
		u^{i+1} &= g \textnormal{ on } \partial \Omega.
	\label{eq:fixed point iteration}
	\end{split}
\end{align}

Taking a closer look at the derivation of \eqref{eq:decoupled PDE} we see only rearrangements and the smoothness of $w$ were used. Hence, for any smooth solution $u$ the \MA equation can also be written as
\begin{align}
- \nabla \cdot \left( \mycof{ D^2 u} \nabla u \right)  = -2f.  \label{eq: analytical fixed point iteration}
\end{align}
This also implies that every solution of a \MA problem is a fixed point of the Picard iteration. It remains to test if the Picard iteration converges to its fixed point and hence yields an approximation of the solution of the \MA equation. 


\section{Discretisation of a Picard Iteration Step}\label{sec: SIPG MA}
Since every iteration \eqref{eq:fixed point iteration} states a Generalised Poisson equation we can apply the derived SIPG method of Section \ref{sec: SIPG} directly: Given $u^i_h\in \mathcal P^k_h$ find $u^{i+1}_h \in \mathcal P^k_h$ satisfying
\begin{align}
 &\myIntX {\Omega} {\nabla v_h \cdot \cof(D_h^2 u_h^{i}) \nabla u_h^{i+1}}  \nonumber\\
 & -\sum\limits_{e \in \edges}\myIntS e { \jump {v_h \average { \cof(D_h^2 u^{i}) \nabla u_h^{i+1}} }}
 - \sum\limits_{e \in \edges}\myIntS e { \jump {u_h^{i+1} \average{ \cof(D^2 u_h^{i}) \nabla v_h} }} \nonumber\\  
 &  +\sum\limits_{e \in \edgesi} \myIntS e { \frac \sigma {|e|} \jump {v_h}  \jump {u_h^{i+1}}}\nonumber\\
    =& - 2 \myIntX {\Omega} {v_h f}
    	 				-\sum\limits_{e \in \edgesb}\myIntS e {g \cof(D^2 u_h^{i}) \nabla v_h \cdot n }
    	 				+\sum\limits_{e \in \edgesb} \myIntS e { \frac \sigma {|e|} v_h g}    \qquad \forall v_h \in  \mathcal P^k_h.
    	\label{eq: sipg iteration}
\end{align}

Now that we derived a new iterated DG method to solve the \MA equation we compare \eqref{eq: sipg iteration} to Newton's method and the method introduced in Section \ref{sec: Brenner method}. 

\input{comparison.tex}

%After this analysis it is not clear how the Picard iteration practically behaves. The next section addresses some first numerical results of the Picard iteration and first obstacles when solving simple problems.  
\section{Challenges for the Picard iterations}
Reviewing existing DG methods for the \MA equation we learned some issues we have to pay attention to, while examining the performances of the Picard iteration. The next subsections treat the most challenging points.
At the beginning there always is the question of a starting point $u_h^0$.
\subsection{Initial Guess}\label{sec: initial guess}
Just as most methods for the \MA equation the fixed point iteration requires an initial guess. Two approaches are very favoured in the literature.
In \cite[Remark 2.1]{DG2006a} has been shown a strong connection between the \MA problem, and  the following Poisson problem
\begin{align}
	\begin{split}
	\triangledown u &= \sqrt{2f} \text{ in } \Omega, \\ 
	u &= g \text{ on }\partial \Omega.
	\end{split}\label{eq: start sqrt_f}
\end{align}
An advantage of this approach is the compliance of boundary conditions.

Alternatively a nested iteration can be utilised. On the coarsest level with a grid width $h_1$ one chooses any convex function as initial guess $u^0_{h_1}$. Often the polynomial $\frac 1 2 ({x_1^2} + {x_2^2}) $ is taken for it is convex and has a low degree. For finer triangulations $\mathcal{T}_{h_{l}}$ the solution of the previously computed solution $u_{h_{l-1}}$is taken as a starting point. Of course both approaches can be combined using the Poisson solution of \eqref{eq: start sqrt_f} on the coarsest grid whereas later taking quasi-interpolants.\\
The second approach is preferable as it takes into account that the robustness of the method may decrease for finer meshes.

Now we are able to perform the Picard iteration on some simple example cases.
\subsection{An additional Penalty Parameter} \label{subsec: add penalty param}
At the beginning the SIPG method is implemented exactly as stated in Section \ref{sec: SIPG MA}. The polynomial degree $k$ of the ansatz and trial space $\mathcal V_h^k$ is taken to be two.
The solution of \eqref{eq: start sqrt_f} serves as an initial guess, the penalty parameter $\sigma$ is chosen to be 30 and the domain $\Omega$ to be the unit square $[0,1]^2$.

For first numerical results we consider the rather simple equation
\begin{align}
	\mydet {D^2 u} &= 1 \text{ in } \Omega, \\ 
	u &= \frac 1 2 (x_1^2 + x_2^2 )\text{ on }\partial \Omega,
\end{align}
with the exact classical solution $\frac 1 2 (x_1^2 + x_2^2 )$. 

\begin{figure}[H]
	\centering
	\includegraphics[trim = 2cm 4cm 1cm 4cm, scale =0.5]{plots/consisctency_first_try.pdf}
	\caption{Relative $L^2$ error on a uniform grid with $h=\frac 1 2$}
	\label{fig: consisctency_first_try}
\end{figure}
However, even for this rather simple example the implementation shows inconsistencies: Given the exact solution which is already contained in $V_h$ as starting point the $L^2$ error increases, as shown in Figure \ref{fig: consisctency_first_try}.

Examining the plots of solutions $u^i_h$ shortly before the method diverges it catches the eye that they have developed some sharp edges although the initial function and the solution are absolutely smooth. In particular the sharp edges of the solution coincide with the edges of the finite element grid. A visual inspection suggest that these solution $u^i_h$ are $C^0$ continuous, but lack higher regularity.

An idea is to force more regularity on the first derivative and thus, implicitly obtain a more smooth solution. This suits to the improvement Neilan introduced for low degrees \cite[Section 5]{Neilan2014} which is also described at the end of Section \ref{subsec: disrete Hessian}. His proposed correction terms vanish as the gradient jump across internal edges tends to zero.
Hence, we add the same normal derivatives penalty term 
\begin{align}
	\sum_{e \in \edgesi} \sigma_g |e |\myIntS e {\jump{ \nabla u} \jump {\nabla v}} \label{eq: grad penalty term}
\end{align}
to the left-hand side of \eqref{eq: sipg iteration}.
This avoids variational crimes by interior jumps of derivatives along interior edges of the triangulation. We implicitly assume that the desired solution is contained in $H^2(\Omega)$, it is not yet clear how this term behaves for a less regular solution.
With the new penalisation of the gradient we observe that an modified implementation is consistent with the previous example. The same test as at the beginning of this section was carried out with the choice $\sigma_g$ equal to $\sigma=30$ and now the $L^2$ errors stayed for 200 iterations numerically zero.
%\todo{Noelle gluecklich machen querschnitt}

Nevertheless, starting over with the initial guess from \eqref{eq: start sqrt_f} the method is unstable and diverges. But the results yet are very remarkable. As shown in Figure \ref{fig: oscillation} the error is oscillating while diverging  to infinity.
\begin{figure}[H]
	\centering
	\includegraphics[trim = 2cm 4cm 1cm 4cm, scale=0.5]{plots/oscillation.pdf}
	\caption{Relative $L^2$ error on a grid with $h=\frac 1 4$ and gradient penalty}
	\label{fig: oscillation}
\end{figure}

\begin{figure}[H]
\begin{subfigure}[b]{.5\textwidth}
	\includegraphics[width=1.\textwidth]{plots/with_penalty_it22.pdf}
	\caption{Solution after 23 steps}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}
	\includegraphics[width=1.\textwidth]{plots/with_penalty_it23.pdf}
	\caption{Solution after 24 steps}
\end{subfigure}
\caption{The solution in two consecutive iterations}
\label{fig: diff iteration}
\end{figure}
Figure \ref{fig: diff iteration} shows two consecutive steps, the error $u-u_{exact}$ is denoted by the surface colour. We see that the solution after 23 steps is mostly red, i.e. it is pointwise greater than the exact solution, while one iteration later the Poisson solution is mostly blue, i.e. it is pointwise smaller than the exact solution. These frames are generic for the behaviour of the intermediate Picard solutions. After a step with a solution $u_h^i$ with $u_h^i \geq u_{exact}$ the solution of the next intermediate function $u^{i+1}_h$ fulfils $u^{i+1}_h \leq u_{exact}$. It looks like the solution is split into two subsequences.  And as we have also seen in the devolution of the error, these subsequences diverge from the exact solution.

%Theoretically it is possible there are two different functions $v,w\in V$ such that $v$ is the solution to the decoupled PDE (cf. \eqref{eq:decoupled PDE}) fixing $w$ and vice versa $w$ the solution of \eqref{eq:decoupled PDE} fixing $v_h$. 
To prevent our algorithm from doing so we couple the two subsequences by a damping. After calculating the new Picard step $u^{i+1}_h$ we combine the current solution with the function calculated one step before by a convex composition $ \alpha u_h^{i+1} + (1- \alpha) u_h^i$ for $\alpha \in (0,1)$. Hence we got a further parameter $\alpha$ to adjust the Picard iteration.

Another parameter sensible to introduce is a parameter to control the ellipticity constant of the used SIPG method.
\subsection{Modification of the Cofactor Matrix}\label{sec: mod cofactor}
For a better ellipticity constant of the Generalised Poisson problem as stated in Section \ref{sec: SIPG MA} it is necessary that the eigenvalues of the diffusion matrix, namely $\mycof{D_h^2 u_h^i}$, are bounded below with some constant $\varepsilon$ (cf. results of Section \ref{sec: SIPG}, in particular Theorem \ref{thm: SIPG stability}).
To fulfil this criterion we add positive multiples of the identity matrix until the smallest eigenvalue becomes $\varepsilon$. Hence, we introduce a modified cofactor matrix
\[ 
	\mycofMod {D^2_h u_h} = \begin{cases}
	\mycof {D^2_h u_h} & \lambda \geq \varepsilon	\\
	\mycof {D^2_h u_h}+ (-\lambda+\varepsilon) Id%\begin{pmatrix} -\lambda+\varepsilon & 0 \\ 0 & -\lambda+\varepsilon \end{pmatrix} 
	& else
	\end{cases}
\]
where $\lambda$ is the minimal eigenvalue of $ \mycof {D^2_h u_h}$ and $Id$ is the $d \times d$-Identity matrix. 
Now we replace all occurrences of $\mycof {D^2_h u_h}$ in \eqref{eq: sipg iteration} by $\mycofMod {D^2_h u_h}$ to stabilise the problem.

The case that a cofactor matrix $\mycof {D^2_h u_h} $ does have a negative eigenvalue or even a complex one implies that also the Hessian of $u_h$ is not positive definite. Hence, $u_h$ is not piecewise convex, we discuss this issue in more detail in the next section.

%Let us recall the geometric interpretation of the second derivative, it indicates the curvature of a function and hence also describes the convexity, concavity respectively.
%One improvement could be the smoothing process during a convexification for it will smooth away all unintended concave curvatures. But probably the result will be still unsatisfiable because convexification does not affect sharp edges at triangle transitions.

\subsection{Convexfication} \label{subsec: convexification}
 The \MA equation in general does not posses a unique solution (cf. Section \ref{sec: Existence and Uniqueness}). Yet for most applications the (unique) convex solution is required. The question is if there exists an appropriate way to oblige the Picard iteration to select only convex solutions.\\
The most intuitive way is to convexify the solution after every step. Thereby, we hope to ensure that the right solution is approximated. There may be additional benefit by a convexification:
Let us recall the geometric interpretation of the second derivative, it indicates the curvature of a function and hence also describes the convexity, concavity respectively. A convexification will smooth away small wiggles and by definition will force the function to be convex.  Thus, a convexification could also improve the approximation quality of second derivatives of our interim solution which we is needed to set up the cofactor of the Hessian for the next Picard step. % aiming for a better approximation of the second derivatives.

Unfortunately convexification is not simple. As there are a lot of results on the convexity of Bernstein polynomials, we choose a Bernstein basis for the ansatz space.
\begin{definition}[Bernstein-B\'ezier form{, \cite[Section.2]{SS2010}}]\label{def: BernsteinBezierForm}
	Any univariate polynomial $p:T \rightarrow \R$ of degree $d$ on a triangle $T$ can be represented in \emph{Bernstein-B\'ezier form} as
\begin{align}
	p(x) = \sum_{\stackrel{i,j,k \in \N}{i+j+k = d}}  c_{ijk} B^d_{ijk}(x),\label{eq: BernsteinBezierForm}
\end{align}
where
\[
	B^d_{ijk}(x) := \frac {d!}{i!j!k!} \beta_1^i \beta_2^j \beta_3^k
\]
are the \emph{Bernstein polynomials} of degree $k$ and $\beta = (\beta_1, \beta_2, \beta_3)\in \R^3$ are the barycentric coordinates of $x \in \R^2$ relative to the triangle $T$.

We call $c=\left(c_{ijk}\right)_{i,j,k \in \N, i+j+k=d}$ also the \emph{B\'ezier coefficents}.
\end{definition}

The B\'ezier coefficients also induce a geometric interpretation. Suppose $T=\langle v_1, v_2, v_3 \rangle$ and the domain points
\begin{align}
	\xi_{ijk} := (i v_1+jv_2 + k v_3) / (i+j+k). \label{eq: domain points}
\end{align} 
Then we call 
\begin{align}
	Q_{ijk} := (\xi_{ijk}, c_{ijk}) \label{eq: control points}
\end{align} the \emph{control points} of a polynomial $p$ defined on $T$. Further we denote the polygon defined by the points $Q_{ijk}$ as the \emph{B\'ezier control polygon}.

\begin{figure}
	\centering
	\input{control_net.pgf}
	\caption{The domain points on a triangle $T=\langle v_1, v_2, v_3 \rangle$ for $d=3$}
	\label{fig: domain points}
\end{figure}
Figure \ref{fig: domain points} shows the distribution of the domain points for the polynomial degree $d=3$.

The benefit of the Bernstein-B\'ezier form is that there can be formed simple conditions ensuring the convexity of the polynomial on a triangle. Namely there is a connection between the convexity of the control polygon and the convexity of the polynomial.
\begin{theorem}[ Connection Control Polygon Convexity{\cite[Theorem 4.6]{Dahmen1991}}]
	The convexity of the control polygon implies the convexity of the represented polynomial.
\end{theorem}
Note that this result holds only for a single triangle and not for a function defined on a grid of triangular cells. Nevertheless, piecewise convexity is already very important for the well-posedness of our intermediate SIPG iteration step (cf. Section \ref{sec: SIPG}) such that it is worth a try to convexify the union of all control polygons.

Due to their importance in computer graphics, there are a lot of convex hull algorithm available. Hence it suggests itself to apply a convex hull algorithm on given Bernstein-B\'ezier coefficients to enforce convexity. However, this approach fails for a convex hull algorithm for it only operates on a set of points and hence does not necessarily produce the same connectivity the control polygon implies. An counterexample where those two polygons differ is given in Figure \ref{fig: diff connectivity}:
\begin{figure}[h]
\begin{subfigure}[b]{.5\textwidth}
	\includegraphics[trim=3cm 8cm 3cm 8cm, width=1.\textwidth]{control_polygon2.pdf}
	\caption{Control polygon}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}
	\includegraphics[trim=3cm 8cm 3cm 8cm, width=1.\textwidth]{convex_hull2.pdf}
	\caption{Lower convex hull}
\end{subfigure}
\caption{The control polygon and lower convex hull of a given set of control points}
\label{fig: diff connectivity}
\end{figure}
While the the control polygon is obviously not convex, the surface of the lower convex hull of the control points is convex only because it connects the inner control points differently.

In the eighties mathematicians derived quadratic conditions on the B\'ezier coefficients equivalent to its convexity\cite{CD1984, Dahmen1991}. Later this conditions were relaxed to a set of linear sufficient conditions, yet loosing necessity. To formulate those conditions we use the difference operator $\Delta_{\mu \nu}$ along the edge $(v_\mu, v_\nu)$. 
Thus for the triangle $T=\langle v_1, v_2,v_3 \rangle$ 
%as seen in Figure \ref{fig: domain points} 
we obtain
	\begin{align*}
		\Delta_{21} c_{ijk} &= c_{i,j+1,k} -c_{i+1, j,k}  \\
		\Delta_{31}^2 c_{ijk} &= c_{i,j,k+2} -2c_{i, j+1,k+1} +c_{i, j+2,k} \\
		\Delta_{12} \Delta_{32} c_{ijk} &= c_{i,j+1,k+1} -c_{i+1, j,k+1} - c_{i,j+2,k} +c_{i+1, j+1,k}.
	\end{align*}
 
 For the following theorems  we always refer to the same setting: We have a triangle $T=\langle v_1, v_2,v_3 \rangle$ and a polynomial $p$ of degree $d$ defined on this triangle $T$. Further the B\'ezier coefficents of $p$ are denoted by $c_{ijk}$.
 
A result due to Chang and Feng \cite{CD1984} are the following quadratic constraints.
\begin{theorem}[Convexity on Triangles{\cite[p.2]{LS2007}}] \label{thm: convex cond quadr}
	A polynomial $p$ is convex on a triangle $T$ if and only if the matrix
	\[
		\begin{pmatrix}
			\Delta_{21}^2 c_{ijk} & \Delta_{31} c_{ijk} \Delta_{32} c_{ijk}\\
			\Delta_{31}c_{ijk} \Delta_{32} c_{ijk} & \Delta_{31}^2 c_{ijk} 
		\end{pmatrix} \in \R^2
	\]
	is nonnegative definite for each $i + j + k =d-2 $.
\end{theorem}
For an overview of relaxations of these conditions, we refer to \cite{SS2010}. An example with 12 inequalities is given in the following theorem.
\begin{theorem}[Sufficient Linear Convexity Conditions, {\cite[Corollary 2.8]{SS2010}}]
\label{thm: convex cond on triangle}
	A polynomial $p$ is convex on a triangle $T$ if its B\'ezier coefficients $c_{ijk}$ satisfy
	\begin{align*}
		&(\Delta_{21} + 2\Delta_{31}) \Delta_{31} c_{ijk} \geq 0, 
		&   (\Delta_{21}^2 + 3\Delta_{21} \Delta_{31} + 2 \Delta_{31}^2) c_{ijk} \geq 0, \\
		& \Delta_{21}(2\Delta_{21} + \Delta_{31})  c_{ijk} \geq 0, 
		&   (2\Delta_{21}^2 + 3\Delta_{21} \Delta_{31} +  \Delta_{31}^2) c_{ijk} \geq 0, \\  
		&(\Delta_{32} + 2\Delta_{12}) \Delta_{12} c_{ijk} \geq 0, 
		&   (\Delta_{32}^2 + 3\Delta_{32} \Delta_{12} + 2 \Delta_{12}^2) c_{ijk} \geq 0, \\
		&\Delta_{32} (2\Delta_{32} + \Delta_{12}) c_{ijk} \geq 0, 
		&   (2\Delta_{32}^2 + 3\Delta_{32} \Delta_{12} +  \Delta_{12}^2) c_{ijk} \geq 0, \\  
		&(\Delta_{13} + 2\Delta_{23}) \Delta_{23} c_{ijk} \geq 0, 
		&   (\Delta_{13}^2 + 3\Delta_{13} \Delta_{23} + 2 \Delta_{23}^2) c_{ijk} \geq 0, \\
		& \Delta_{13}(2\Delta_{13} + \Delta_{23})  c_{ijk} \geq 0, 
		&   (2\Delta_{13}^2 + 3\Delta_{13} \Delta_{23} +  \Delta_{23}^2) c_{ijk} \geq 0   
	\end{align*}
	for all $i + j + k = d-2 $.
\end{theorem}
Note that the latter conditions of Theorem \ref{thm: convex cond quadr} and Theorem \ref{thm: convex cond on triangle} ensure convexity on a single triangle $T$. That would suffice for piecewise polynomials contained in $C^1$ because for them, unlike to splines contained in $C^0$ convexity on each triangle implies global convexity \cite[Theorem 3.1.]{SS2010}. To patch this matter Schumaker and Speleers introduce further conditions making sure $C^0$ splines are convex across triangle boundaries in \cite{SS2014}.
\begin{theorem}[Sufficient Conditions across Edges,{ \cite[Theorem 3.6]{SS2014} }]\label{thm: convex cond across edge}
	Let $p \in \mathcal P^d_h \cap C^0(\Omega)$ be a piecewise polynomial being convex on every triangle $T \in \triang$. Suppose its B\'ezier coefficients for every interior edge $e =(v_\kappa, v_\mu)$ fulfil 
	\begin{align}
		{\hat c_{i,j,1}}  \geq  \beta_1^c c_{i+1, 0,j} +\beta_2^c c_{i,1,j} + \beta_1^c c_{i, 0,j+1}, \qquad i+j=d-1, \label{eq: convexity across edge}
	\end{align}
where  $\{c_{i,j,k}\}_{i+j+k=d}$ and $\{ {\hat c_{i,j,k}}\}_{i+j+k=d}$ are the B\'ezier coefficients of $p$ relative to the two triangles $T_1 = \langle v_\kappa, v_\lambda, v_\mu \rangle$ and $T_2 = \langle v_\kappa, v_\mu, v_\nu \rangle$ sharing the edge $e$, and $(\beta_1^c,\beta_2^c,\beta_3^c)$ are the barycentric coordinates of $v_\nu$ with respect to $T_1$. Then $p$ is convex.
\end{theorem}

\begin{figure}[H]
	\input{triangle_convexity_condition.pgf}
	\caption{Two triangles and B\'ezier control points for polynomials of degree $k=2$ \\($T_1 = \langle v_\kappa, v_\lambda, v_\mu \rangle$ and $T_2 = \langle v_\kappa, v_\mu, v_\nu \rangle$)}
	\label{fig: convexity condition}
\end{figure}
Figure \ref{fig: convexity condition} shows the domain points $\xi_{ijk}$ (cf. \eqref{eq: domain points}) corresponding to the B\'ezier coefficients of the scenario of Theorem \ref{thm: convex cond across edge} for the case $d=2$.

 Let us consider the condition
\[
		{\hat c_{101}} \geq \beta_1^c c_{200} +\beta_2^c c_{110} + \beta_1^c c_{101}	
\]
Geometrically in the case of $d=2$ this ensures the control point $Q_{101}$ (cf. \eqref{eq: control points})is above the plane where the triangle $\langle Q_{200}, Q_{110}, Q_{101} \rangle$ is embedded. %Note that the first two coordinates are the coordinates of the domain points. Thus, we can identify the 
Or in particular the surface consisting of the triangle $\langle Q_{200}, Q_{110}, Q_{101} \rangle= \langle \hat Q_{200}, Q_{110}, \hat Q_{110} \rangle$  and the triangle $\langle \hat Q_{200}, \hat Q_{110}, \hat Q_{101} \rangle$ is convex.

For quadratic splines Schumaker and Speleers prove the inversion of Theorem \ref{thm: convex cond across edge} on convex domains $\Omega$, namely if for every interior edge its coefficients satisfy \eqref{eq: convexity across edge} then the corresponding spline is convex \cite[Corollary 3.10]{SS2014}.
This result does not generalise for higher degrees. In fact they give a counterexample for degree $k = 3$ \cite[Example 3.11]{SS2014}.

Now we apply the new insights to the solution of the Generalised Poisson problem stated in Section \ref{sec: SIPG MA}.
Given the DG solution of the Generalised Poisson problem $u^{gp}_h$ we seek for a convex spline minimising the error at the B\'ezier control points, i.e. we want to find the B\'ezier coefficients $c$ minimising
\begin{align}
		\lVert A c - b \rVert_2, \qquad \text{ such that } Cc \geq 0, \label{eq: convex lsq}
\end{align}
where $A$ is the matrix evaluating the piecewise polynomial induced by the B\'ezier coefficients $c$ corresponding  at the domain points (cf. \eqref{eq: domain points}), $b$ are the function values of $u^{gp}_h$ at the B\'ezier control points and $C$ is the matrix containing the conditions ensuring convexity on the whole domain.

However, solving a linearly constrained least squares problem is nontrivial. So, when using this approach one should ensure that a convexification is justified. Also, one has to keep in mind that the constraints in the matrix $C$ are sufficient but not necessary. We discuss in Chapter \ref{ch:NumericalResults} whether this approach fulfils our requirements on a convexificiation.

\section{The Picard Iteration Algorithm} \label{sec: Picard Iteration Algo}

Combining the suggestions of the last sections we obtain a method to carry out the fixed point iteration working as Algorithm \ref{alg: final} describes.

\begin{algorithm}[H]
\begin{algorithmic}
\Require triangulation \triang, desired mesh width $h$, maximimal number of intermediate steps $i_{max}$
\State $u_0\gets $ solution of  $
	\triangle u = \sqrt{2f} \text{ in } \Omega $ with $
	u = g \text{ on }\partial \Omega$ \Comment initialisation
\While {$h < H$}
	\State $i \gets 0$
%	\While {$|u_i-u_{i-1}| < \varepsilon$}
	\While {$i < i_{max} $}
		\State $u_i \gets$ solution of \ref{sec: SIPG} with modified cofactor matrix (\ref{sec: mod cofactor})
		\State (convexify)
		\State $u_i \gets \alpha u_{i-1} + (1-\alpha)u_i $ \Comment convex combination
		\State $i \gets i+1$
	\EndWhile
	\State $h, \triang \gets h/2, \triangFine$
\EndWhile
\end{algorithmic}
\caption{Picard Iteration Algorithm to Solve the \MA Equation}
\label{alg: final}
\end{algorithm}

The step convexify is put into brackets for it is not clear whether a convexification is necessary and actually helps the convergence. A lot of the methods mentioned in the Chapters \ref{ch:MongeAmpereEq} and \ref{ch:DGMongeAmpere} work without an explicit convexification, although most require a convex initial guess. 
Additionally the approach described in Section \ref{subsec: convexification} is intricately to implement, significantly increases the computation costs and may modify functions even though they already are convex. We note, that for the well-posedness of the used SIPG method we need the cofactor matrix to be positive definite which is equivalent to the piecewise convexity of the last iteration step. However, our modification of the cofactor matrix  may avert the problems arising for slightly non-convex intermediate solutions.
