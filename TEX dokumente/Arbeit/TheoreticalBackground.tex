This chapter discusses the mathematical foundation for the numerical schemes which are presented in later chapters.
At the beginning of this chapter we repeat the basic definitions and notation for an analysis of partial differential equations as they are introduced for example in \cite[Introduction]{Evans1998}. Afterwards we introduce the finite element, as well as a discontinuous Galerkin method, for the latter we provide also a proof of its well-posedness. To prepare later calculations we end this chapter with some algebraic identities about the Hessian.

\section{ Preliminaries and Notation}
Let $\Omega \subset \R^d $ be an open, bounded area and $\partial \Omega$ its boundary.
\begin{definition}[Partial Differential Equation{\cite[Introduction]{Evans1998}}]
 A \emph{partial differential equation} (PDE) of order $k\in \N$ is an expression of the form
\begin{align}
	F(D^k u(x), D^{k-1}u(x), \dots, Du(x), u(x), x) = 0, \; (x \in \Omega)\label{eq:general PDE}
\end{align}
where $F:\R^{n^k} \times \R^{n^{k-1}} \times \dots \times \R^n \times \R \times \Omega \rightarrow \R $ is given and $u:\Omega \rightarrow \R$ is unknown.
\end{definition}
We focus in this thesis on second-order PDEs for our main PDE, namely the \emph{\MA equation} which we will introduce later is of second order. 
\\Beside the order there exist further properties to categorise PDEs:
\begin{definition}[Categories of PDEs{\cite[Introduction]{Evans1998}}] \label{def: categories of PDEs}
	Given functions  $f:\Omega \rightarrow \R$ and $a_{\alpha}:\Omega \rightarrow \R$ or $a_{\alpha}:\R^{n^{k-1}} \times \dots \times \R^n \times \R \times \Omega$ respectively, for $\alpha \in \N^{k}, |\alpha| \leq k$, a PDE of order $k$ is called
	\begin{enumerate}[(i)]
		\item \emph{linear} if it can be written in the form
		\[
			\sum_{|\alpha| \leq k} a_{\alpha} (x) D^{\alpha} u(x) = f(x).
		\] 
		
%		\item \emph{semilinear} if it can be written in the form
%		\[
%			\sum_{|\alpha| = k} a_{\alpha}(x) D^{\alpha} u + a_0(D^{k-1}u, \dots, Du, u, x)= f(x),
%		\]	
%		i.e. a semilinear PDE is nonlinear in the unknown function, but linear in all partial derivatives.
		
		\item \emph{quasilinear} if it can be written in the form
		\[
			\sum_{|\alpha| = k} a_{\alpha}(D^{k-1}u, \dots, Du, u, x) D^{\alpha} u + a_0(D^{k-1}u, Du, u, x)= f(x),
		\]	
		i.e. a quasilinear PDE is nonlinear in (at least) one lower derivate, but linear in the highest order derivates.
		
		\item \emph{fully nonlinear} if it depends nonlinearly upon the highest order derivatives.
	\end{enumerate}
\end{definition}
The standard example for linear PDEs is the \emph{Poisson equation} $-\triangle u = f$.
% a semilinear PDE is for example the \emph{nonlinear Poisson equation} $-\triangle u = f(u)$.
A well-known example for quasilinear PDEs are equations of the type $\nabla \cdot (A(u) \nabla u) = f$, where $A: \R^d \rightarrow \R^{d \times d}  $. The \MA equation $\mydet {D^2 u} = f$ is part of the last category, the nonlinear PDEs. 

Another important property to classify second-order PDEs is ellipticity. 
\begin{definition}[Elliptic PDE{, \cite[p.207]{FGN2013}}]
	A fully nonlinear second order PDE is called \emph{elliptic} if its operator satisfies
	\begin{align}
		F(A,p,z,x) \leq F(B,p,z,x) \label{eq: ellipitic PDE}
	\end{align}
for all $x \in \Omega, z \in \R, p \in \R^d$ and $A,B \in \R^{d \times d}_{sym}$  with $A-B$ positive semidefinite.

%This notion is a natural extension of the ellipticity concept for first order PDEs. Interpreting a first order PDE operator as a function depending on $D^2u, Du, u, x$, \eqref{eq: ellipitic PDE} characterises an elliptic PDE.
\end{definition}

Searching for a function $u:\Omega \rightarrow \R$ with 
\begin{align}
u=g \text{ on } \partial \Omega \label{eq: boundary}
\end{align}
for some $g:\partial \Omega \rightarrow \R$ is called a \emph{Dirichlet-boundary problem}. The function $u:\Omega \rightarrow \R$ fulfilling \eqref{eq:general PDE} and \eqref{eq: boundary} is called a classical solution of the problem. 

There are problems such that no classical solution exists. A PDE of order $k$ requires its classical solution to be at least $k$ times differentiable, which is a very strong demand. To admit less regular solutions one aims for a weaker notion of a solution. Before doing this we specify some function spaces.

\begin{definition} [Function Spaces and Norms]\label{def: function spaces and norms}
Accordingly to the majority of the literature we refer by $W^{s,p}(\Omega)$ to the H\"older space, a set consisting of all $L^p(\Omega)$ functions whose distributional derivates up to order $s$ are contained in $L^ p(\Omega)$.
In the special case $p=2$ they are referred to as Sobolev spaces $H^s(\Omega):=W^{s,2}(\Omega)$. An additional subscript $0$ denotes the set where all functions are zero at the boundary, i.e. $H^s_0 :=\{f \in H^s(\Omega); f(x)=0 \; \forall x \in \partial \Omega\}$.

The corresponding function space norms are defined by
\[
	\norm f _{W^{s,p}(\Omega)} = \left( \sum_{|\alpha| \leq s} \norm{D^\alpha f}^p_{L^p(\Omega)}\right)^{\frac 1 p }, \text{   especially } \norm f _{H^{s}(\Omega)}=\left( \sum_{|\alpha| \leq s} \norm{D^\alpha f}^2_{L^2(\Omega)}\right)^{\frac 1 2 }
\]
and the $H^s$ semi-norms
\[ 
 	|f|_{H^{s}(\Omega)}=\left( \sum_{|\alpha| = s} \norm{D^\alpha f}^2_{L^2(\Omega)}\right)^{\frac 1 2 }.
\]
%If not stated otherwise $\bilin f g$ always refer to the standard $L^2$ scalar product 
%\[
%	\bilinLTwo{f}{g} = \myInt_{\Omega } f g.
%\]
For a normed function space $X$ we denote its dual by $X'$ and by $\bilin{\cdot}{\cdot}$ the pairing between $X$ and $X'$.

\end{definition}
By this definition it is clear that the $H^0$ norm and the $L^2$ norm conincide. 

Let $T$ be the triangle in $\Omega$ spanned by the points $v_0, v_1, v_2$. We also write 
\[
	T= \langle v_0, v_1, v_2 \rangle := \{\beta_0 v_0+ \beta_1 v_1 +\beta_2 v_2; \sum_{i=0}^2 \beta_i =1, \beta_i \geq 0, i= 0,1,2\}.
\]
We define the \emph{diameter} $diam(T)$ of a triangle $T$ as the diameter of the smallest circle containing $T$.

To discretise our domain $\Omega$ we subdivide it into simplices, more precisely we divide it into triangles.
\begin{definition}[Triangulation, {\cite[Definition 5.1]{Braess2003}}]
	We call a finite partition of $\Omega$ into triangles $\{T_1, \dots, T_n\}$ a feasible \emph{triangulation} $\mathcal{T}$, such that
	\begin{enumerate}[(i)]
		\item	$\overline \Omega = \bigcup_{T \in \triang} T$
		\item If $T_i \cap T_j$ is a point for $i \neq j$ , it is a vertex of both $T_i$ and $T_j$.
		\item If $T_i \cap T_j$ is more than one point for $i \neq j$, it is an edge of both $T_i$ and $T_j$.
	\end{enumerate}
	Let $h$ denote the maximal diameter $h:= \operatorname{max}\limits_{T\in \mathcal T} diam(T)$ in the triangulation $\mathcal T$, we then also refer to $\mathcal{T}$ by $\triang$. We call a triangulation \emph{regular} if every element is congruent.
\end{definition}
Note that this definition requires $\Omega$ to be a polygonal domain. However, there are extensions to domains with curved boundaries: In those triangles lying at the boundary are replaced by \quoting{triangles} which have curved sides. 

\begin{figure}[H]
\input{grid.pgf}
\caption{A regular triangulation of the unit square}
 \label{fig: triangulation}
\end{figure}

Figure \ref{fig: triangulation} shows a triangulation of the unit square. Since the edges uniquely define a triangulation of $\Omega$, we later also use the terms grid and mesh to refer to a subdivision into triangles.
An edge $e$ is called an interior edge if $e=T^+ \cap T^-$ holds for some $T^+,T^- \in \triang$, whereas it is called a boundary edge if $e \subset \partial \Omega$.
We divide the edges $\edges$ into the subsets $\edgesi$ and $\edgesb$ containing all interior edges and boundary edges, respectively. 
When we refer to a normal of an edge we always refer to an outward directed normal of one of the adjacent triangles. If it is not clear from the context to which triangle is referred we clarify that with a superscript.

On this basis we define some piecewise spaces.
\begin{definition}[Broken Spaces]
We define function spaces associated to a triangulation $\triang$ by
\begin{align*}
	H^s(\Omega;\triang) &:= \{f \in L^2(\Omega); f|_T \in H^s(T) \text{ for every } T \in \triang\}, \\
	\text{and }
	W^{s,p}(\Omega;\triang) &:= \{f \in L^p(\Omega); f|_T \in H^s(T) \text{ for every } T \in \triang\}.
\end{align*}	
\end{definition}

\begin{definition}[Piecewise Polynomial Spaces] \label{def: piecewise polySpace}
	Further we denote the finite-dimensional space of piecewise polynomials by
\[	
	\mathcal P^k_h = \{ f \in L^2(\Omega); f \arrowvert_T \textnormal{ is a polynomial of (total) degree } \leq k \textnormal{ for every } T \in \triang\}.
\]
\end{definition}

For later analysis we need the approximation properties of $\mathcal P_h^k$ proven in \cite[Chapter 4]{BS2002}, as well as standard trace and inverse estimates as proven in \cite[Section 1.6 and Section 4.5]{BS2002}.
\begin{lemma}[Approximation properties of $\mathcal P_h^k${,\cite[Theorem(4.4.20)]{BS2002}}]  \label{la: approximation properties}
Let $s,m \in \N$ with $0 \leq m \leq s\leq k+1$. Then for any $u \in H^s(\Omega)$, there exists $v_h \in \mathcal P^k_h$ such that 
\[
	\left( \sum_{T \in \triang} \norm{u-v_h}^2_{H^m(T)} \right)^{\frac 1 2} \leq C h^{s-m} \norm{u}_{H^{s}(\Omega)},
\]
where $C$ is a constant depending only on the mesh, but not in $h$ or $u$.
\end{lemma}
\begin{lemma}[Trace estimate{,\cite[Theorem(1.6.6)]{BS2002}}]\label{la: trace estimate}
	Given any quasi-uniform trianguliaton $\triang$, there holds for $ v \in H^1(\Omega;\triang)$
	\begin{align*}
	%	\norm{v}_{L2 (\partial T)}^2 \leq C\left(\frac 1 {h_T} \norm{v}_{L2(T)}^2  + h_T \norm{\nabla v}_{L2(T)}^2\right) \qquad \forall v \in H^1(T).
	\sum_{e \in \edges} \LTwonormE{v}^2 
		\leq C_1
		 \sum_{T \in \triang} \LTwonormT{v} \HOnenormT{v}
	\leq 
		C_2 \sum_{T \in \triang} \left(\frac 1 {h} \LTwonormT{v}^2  + h \HOnenormT{v}^2\right).
	\end{align*}
	 where $C_1,C_2$ are positive constants depending only on the mesh but not on $v$.
\end{lemma}
Note that the second inequality follows from Young's inequality $\sqrt{a} \sqrt{b} \leq \varepsilon a + \frac 1 \varepsilon b$ for arbitrary $a,b \in \R^+$ and $\varepsilon \geq 0$. This inequality can be easily verified taking the root of the inequality $ab \leq \varepsilon^2 a^2 + 2ab + {\frac 1 \varepsilon}^2b^2 = \left( \varepsilon a + \frac 1 \varepsilon b\right)^2$.

\begin{lemma}[Inverse Estimate{, \cite[Lemma 2.4]{BGN+2011}}]\label{la: inverse estimate}
	For any piecewise polynomial $v \in \mathcal P_h^k$ there holds
	\[
	\norm v_{H^m(T)} \leq C h^{s-m} \norm v_{H^s(T)} \qquad \forall T \in \triang
	\]
\end{lemma}

To formulate a method capable of handling discontinuities along triangle edges we introduce edge operators.   
\begin{definition}[Edge Operators] \label{def: edge operators}
We denote the \emph{normal jump} of  a piecewise smooth function $u_h:\Omega \rightarrow \R$ across an edge $e \in \edges$ by
\begin{align*}
	\jump {u_h}&:= 
	\begin{cases}
		u_h^+  \mathbf n^+ + u_h^- \mathbf n^-  &\text{ if } \partial T^+ \cap \partial T^- = e \text{ for some }T^+,T^- \in \triang, \\
		u_h \mathbf n 	 &\text{ if } \partial T \cap \partial \Omega = e \text{ for some } T \in \triang,
	\end{cases}	
\end{align*}
where $n^\pm$ are the normals with respect to $T^\pm$ and  $u_h^\pm(x) := \lim_{\varepsilon \rightarrow 0} u_h(x-n^\pm \varepsilon)$.
Likewise we define the normal jump of a piecewise smooth function $u_h:\Omega \rightarrow \R^2$ across an edge $e \in \edges$ by
\begin{align*}
	\jump {u_h}&:= 
	\begin{cases}
		u_h^+ \cdot \mathbf n^+ + u_h^-\cdot  \mathbf n^-  &\text{ if } \partial T^+ \cap \partial T^- = e \text{ for some }T^+,T^- \in \triang,\\
		u_h\cdot \mathbf n 	 &\text{ if } \partial T \cap \partial \Omega = e \text{ for }T\in \triang.
	\end{cases}	
\end{align*}
Here $\cdot$ denotes the scalar product.
Note that the jump of a scalar is a vector, whereas the jump of a vector is a scalar.

The \emph{average} of a piecewise smooth function $u_h:\Omega \rightarrow X$ for every $X$ across an edge $e \in \edges$ is defined by
\begin{align*}
	\average {u_h} &:= 
	\begin{cases}
	\frac 1 2 \left(u_h^+ + u_h^-\right) &\text{ if } \partial T^+ \cap \partial T^- = e \text{ for some }T^+,T^- \in \triang, \\
	 u_h &\text{ if } \partial T \cap \partial \Omega = e \text{ for }T\in \triang.
	\end{cases}
\end{align*}
\end{definition}

We end this section with the note that we use a subscript $h$ to indicate a piecewise evaluation or interpretation.  Subsequently differential operators with a subscript $h$ imply that differentiations were carried out seperately on every triangle, for example the piecewise gradient is defined by $\nabla_h f |_T = \nabla f |_T \; \forall T \in \triang$. 



\section{Finite Element Method}
Before we discuss numerical schemes for the \MA equation we shortly recall how a finite element method works, for a deeper insight in finite element methods we recommend \cite{Braess2003, BS2002}. Since the finite element method bases on a variational formulation we introduce the method talking about the well-known example of the generalised Poisson equation. 

%\subsection{The Poisson Problem}

\begin{definition}[Generalised Poisson Problem] \label{def: General Poisson Problem}
The \emph{generalised Poisson problem} is finding a function $u:\Omega \rightarrow \R$ such that 
\begin{align}
	-\nabla \cdot (A \nabla u) = f \qquad &\text{ in }\Omega \label{eq: poisson eq} \\
	u = g \qquad &\text{ on } \partial \Omega    \label{eq: poisson bc}
\end{align}
for $ A:\Omega^d \rightarrow \R^{d \times d}$ symmetric and functions $f,g:\Omega \rightarrow \R $. 
Because of the physical context of this PDE the matrix $A$ is also referred to as \emph{diffusion matrix}.
\end{definition}

First, we note that we can reduce all general Poisson problems with Dirichlet boundary data to problems with \emph{homogeneous boundary data}, i.e. $g(x) = 0$ for all $x \in \partial \Omega$: Let us assume $\tilde g$ is defined on all of $\Omega$ with $\tilde g = g$ on $\partial \Omega$. Due to the linearity of the differential operators we have that $v:=u-\tilde g$ satisfies
\begin{align}
	-\nabla \cdot (A \nabla v) &= f + \nabla \cdot (A \nabla \tilde g)  \qquad &&\text{ in }\Omega \label{eq: poisson homogeneous eq} \\
	v &= 0 \qquad &&\text{ on } \partial \Omega.    \label{eq: poisson homogeneous bc}
\end{align}
Hence, we can restrict ourselves to the case with homogeneous boundary data.

We are now able to set up a \emph{weak formulation} of the generalised Poisson problem based on variational principles, this formulation is the key element for deriving a finite element method.

Due to the main theorem of variational analysis every classical solution to \eqref{eq: poisson eq} fulfils 
\begin{align}
	-\myIntX \Omega {\varphi \nabla \cdot (A \nabla u)}=
	 \myIntX \Omega {\varphi f} \qquad \forall \varphi \in C_0^\infty(\Omega), \label{eq: variational form}
\end{align}
and vice versa.
Integration by parts yields to
\begin{align*}
	\myIntX \Omega {\nabla \varphi  \cdot A\nabla u}
		%-\myIntS  {\partial \Omega} { \varphi (A\nabla u) \mathbf{n} }
	= \myIntX  \Omega { \varphi f } \qquad \forall \varphi \in C_0^\infty(\Omega). %\label{eq: FE integration by parts}
\end{align*}
The last statement can be reformulated to the so called variational form 
\begin{align}
a(\varphi,u)  = b(\varphi) \qquad \forall \varphi \in C_0^\infty(\Omega). \label{eq: variational formulation}
\end{align}
with the bilinear form $a:C_0^\infty(\Omega) \times X \rightarrow \R$
\begin{align*}
a(\varphi,u) = \myIntX  \Omega { \nabla \varphi  \cdot A\nabla u }
	% -\myIntS  {\partial \Omega} { \varphi (A\nabla u) \mathbf{n}}
\end{align*}
where $X$ is a function space we define in a moment and the functional $b:C_0^\infty(\Omega) \rightarrow \R$
\begin{align*}
 b(\varphi)  = \myIntX  \Omega { \varphi f}.
\end{align*}


\begin{definition}[Weak Solution]
	Functions satisfying \eqref{eq: variational formulation} are called \emph{weak solutions} of \eqref{eq: poisson eq}.
\end{definition}
Note that due to the lack of regularity weak solutions may not be restricted to the boundary of $\Omega$. However one can reformulate boundary conditions for weak solution with trace operators \cite[Section 5.5]{Evans1998}.

The generalised Poisson problem is well-posed in the sense that it has a unique weak solution if $A$ is positive definite and $f\in L^2(\Omega)$. This is mainly due to the ellipticity of the PDE (and as a consequence thereof the ellipticity of $a$). Thus, it is possible to apply the Lax-Milgram theorem and a maximum principle. For an analysis we refer the interested reader to \cite[Chapter~6]{Evans1998}.

On the left-hand side in the weak notation only first derivatives of $u$ appear, hence \eqref{eq: variational formulation} is also applicable for functions $u$ which are not twice differentiable. It turns out that the previously defined Sobolev spaces are well-suited candidates for the space $X$ containing weak solutions \cite[Chapter 1]{BS2002}. Therefore we search for the solution of the infinite-dimensional problem: Find $u\in H_0^1(\Omega)$ such that  $a(\varphi,u)  = b(\varphi) \;\forall \varphi \in C_0^\infty(\Omega)$ holds. % $\forall \varphi \in C^\infty(\Omega)$.

The main idea of the finite element method is to approximate the two function spaces, namely the spaces $H_0^1(\Omega)$ and $C_0^\infty(\Omega)$ containing $\varphi$ and $u$, respectively by finite dimensional spaces $W_h$ and $V_h$. For those spaces should hold a analogous equation as we have in the variational form stated in \eqref{eq: variational formulation}.

The finite space $W_h$ is called \emph{test space} and its elements are called \emph{test functions}. $V_h$ is referred to as \emph{ansatz} or \emph{trial space}, the contained functions accordingly \emph{ansatz} or \emph{trial functions}. According to the boundary conditions of the original function spaces we demand for the finite spaces that $u_h|_{\partial \Omega} = 0$ for every $u_h \in V_h \cup W_h$.

Usually these limited function spaces are based on a discretisation of $\Omega$ and are only piecewise smooth. Consequently, we have to replace all operators appearing in $a$ by operators defined on $W_h \cup V_h$. We denote these discretised versions using the subscript $h$.

With these preliminaries we can define the finite element methods as searching for $u_h \in V_h$ such that 
\begin{align}
a_h(w_h,u_h) = b_h(w_h) \qquad \forall w_h \in W_h. \label{eq: FE variational formulation}
\end{align}
with the bilinearform  $a_h:W_h \times V_h \rightarrow \R$
\begin{align*}
a_h(w_h,u_h)  = \myIntX  \Omega { \nabla w_h  \cdot A\nabla u_h} %-\myIntS  {\partial \Omega} { w_h (A\nabla u_h) \mathbf{n}}
\end{align*}
and the function $b:W_h \rightarrow \R$
\begin{align*}
b_h(w_h) = \myIntX  \Omega { w_h f}.
\end{align*}

This general proceeding is called a \emph{Galerkin approach}. Choosing $W_h = V_h$ yields to the so-called \emph{Galerkin methods}, otherwise the methods are referred to as \emph{Petrov-Galerkin methods}.

To construct $u_h$ we first choose a basis $B_W$ of $W_h$ and a basis $B_V$ of $V_h$.
Since the left-hand side is linear in $w_h$ we only need to check the equations in \eqref{eq: FE variational formulation} for all $w_h \in B_W$. 
Furthermore we can express $u_h$ as the linear combination of basis functions, namely $\sum_{p \in B_V} \mathbf{c}_p \; p$ and hence, we can rewrite \eqref{eq: FE variational formulation} as a linear system of equations $M \mathbf{c} = \mathbf{b}$ where  $\mathbf{c}$ is the unknown coefficient vector of $u_h$ to the basis $B_V$.  The matrix $M$ sometimes is named the \emph{system} or \emph{stiffness matrix}.

Altogether a finite element method for the generalised Poisson equation is defined by the bilinearform $a$ and the functional $b$ together with the choice of $W_h$ and $V_h$. Its computations consist of the assembling process where the matrices $M$ and the right-hand side vector $b$ are computed and the solving process of the resulting linear system of equations.\\
The error made by restriction to finite elements is characterised by the following Lemma.
\begin{lemma}[C\'ea Lemma{, \cite[(2.8.5)]{BS2002}}] \label{la: Ceas lemma}
	Let the bilinear form $a$ be $V$ elliptic with $H_0^m \subset V \subset H^m(\Omega) $, i.e. $0 < \alpha \leq C$ exist with
	\[
		a(u,u) \geq \alpha  \norm u ^2_V \text{ and } |a(v,u)| \leq C \norm u _V \norm v _V.
	\]
	Then for the solution $u_h$ of \eqref{eq: FE variational formulation}  holds
	\begin{align}
		\norm {u - u_h}_V \leq \frac C {\sqrt \alpha} \inf\limits_{v_h \in V_h} \norm {u-v_h}_V.
	\end{align}
\end{lemma}
This result shows that the quality of the finite element solution depends on the one hand on the problem parameters $\alpha$ and $C$, but on the other hand particularly on the approximation properties of $V_h$. A typical choice for the function spaces $W_h$ and $V_h$ are piecewise polynomials spaces which have good approximation properties and are easy to handle.

\section{Discontinuous Galerkin} \label{sec: SIPG}
A recent idea is to to include discontinuous functions when choosing the test and ansatz spaces. This approach arose in the context of hyperbolic PDEs whose solutions eventually contain discontinuities, later it was also applied to elliptic PDEs \cite{ABC+2002}. Although the generalised Poisson problem is not hyperbolic and its solutions are continuous we use this example to explain the issues arising when the finite element spaces are extended to discontinuities. The crux is the handling of function evaluations along discontinuities. Note that for discontinuous ansatz spaces it is no longer true that they are subspaces of $H_0^1(\Omega)$ which contains the weak solution of the generalised Poisson problem. That implies that C\'eas Lemma \ref{la: Ceas lemma} is not valid. Elements which include ansatz space not contained in $H_0^1(\Omega)$ are also referred to as non-conforming elements.

%We recall our triangulation $\mathcal{T}_h$ of $\Omega$. 
If we modify the derivation of the bilinear form defining the finite element method we can develop bilinear forms defined on the function space $V_h = \mathcal P_h^k$ (cf. Definition \ref{def: piecewise polySpace}). The method that we derive asserts the boundary conditions not by the choice of function spaces but force them implicitly in the bilinear form. Hence, we do not assume necessarily homogeneous boundary conditions anymore.

Due to the fact that we have discontinuities only along edges we perform the integration by parts in \eqref{eq: variational form} piecewise on every triangle leading to
\begin{align}
	a(v, u) = & \sum_{T \in \triang} \myIntX T {\nabla v \cdot A \nabla u }
		- \sum_{T \in \triang} \myIntS  {\partial T} { v A \nabla u \cdot \mathbf n}.
\end{align}
Since two adjacent elements share the same edge only with opposite directed normal vectors we can rewrite the last term as
\begin{align*}
\sum\limits_{T \in \triang}\myIntS  {\partial T} { v A \nabla u \cdot \mathbf n }
= &\sum\limits_{e \in \edgesi}\myIntS e { \left( v^+ A^+ \nabla u^+ \cdot \mathbf n^+ + v^- A^- \nabla u^- \cdot \mathbf n^- \right)} \\
& + \sum\limits_{e \in \edgesb}\myIntS e { v A \nabla u \cdot \mathbf n},
\end{align*}
where $h^\pm $ is $h$ evaluated in one of the two adjacent elements $T^\pm$ with $T^+ \cap T^- = e$. With the identity $\mathbf n^- = -\mathbf n^+$ we can extend
\begin{align*}
	&v^+ A^+ \nabla u^+ \cdot \mathbf n^+ + v^- A^- \nabla u^- \cdot \mathbf n^- \\
		= & \phantom{+} v^+ A^+ \nabla u^+ \cdot \mathbf n^+ 
		     + \frac 1 2  v^+ A^- \nabla u^- \cdot \mathbf n^- + \frac 1 2 v^+ A^- \nabla u^- \cdot \mathbf n^+ \\
		& + \frac 1 2  v^- A^+ \nabla u^+ \cdot \mathbf n^+ + \frac 1 2 v^- A^+ \nabla u^+ \cdot \mathbf n^-
		   + v^- A^- \nabla u^- \cdot \mathbf n^-.
\end{align*}
Splitting up first and last term, we find this equals to
\begin{align*}
		 & \phantom{+} \frac 1 2 v^+ A^+ \nabla u^+ \cdot \mathbf n^+ 
		     + \frac 1 2  v^+ A^- \nabla u^- \cdot \mathbf n^- + \frac 1 2  v^- A^+ \nabla u^+ \cdot \mathbf n^+ + \frac 1 2 v^- A^- \nabla u^- \cdot \mathbf n^- \\
		& + \frac 1 2  v^+ A^+ \nabla u^+ \cdot \mathbf n^+  + \frac 1 2 v^+ A^- \nabla u^- \cdot \mathbf n^+ +\frac 1 2 v^- A^+ \nabla u^+ \cdot \mathbf n^- + \frac 1 2 v^- A^- \nabla u^- \cdot \mathbf n^-\\
		    	  = & \phantom{+} \frac 1 2 \left(v^+ + v^- \right) \left(A^+ \nabla u^+ \cdot \mathbf n^+ + A^- \nabla u^- \cdot \mathbf n^- \right) \\
  &+  v^+ \frac 1 2  \left(A^+ \nabla u^+ + A^- \nabla u^-\right) \cdot \mathbf n^+ + v^- \frac 1 2 \left(A^+ \nabla u^+ + A^- \nabla u^-\right) \cdot \mathbf n^- \\
  = &  \jump {\average v  A \nabla u}+ \jump {v \average{ A \nabla u}}.
\end{align*}
Therefore the weak formulation can be written as $a(v,v) = l(v)$ with 
\begin{align*}
  a(v, u) = & \sum\limits_{T \in \triang} \myIntX  T { \nabla v \cdot A \nabla u} \\
	& - \sum\limits_{e \in \edgesi}\myIntS {e} {\left( \jump {\average v A \nabla u} + \jump {v \average{ A \nabla u}} \right)}\\
& - \sum\limits_{e \in \edgesb}\myIntS {e} {v A \nabla u \cdot \mathbf n}
\end{align*}
and
\[
l(v) = \sum_{T \in \triang} \myIntX  T { v f}.
\]
Due to the smoothness of the exact solution $u$ we neglect the jump in $A \nabla u$, i.e. $\jump A \nabla u =0$, and find
\begin{align*}
 a(v, u) = & \sum\limits_{T \in \triang} \myIntX  T { \nabla v \cdot \left(A \nabla u\right)} %\\
	- \sum\limits_{e \in \edgesi} \myIntS e {\jump{ v \average{ A \nabla u}}} %\right)
	\\
& - \sum\limits_{e \in \edgesb}\myIntS e { v A \nabla u \cdot \mathbf n}.
\end{align*}

To get a symmetric system matrix we symmetrise our bilinear form $a$ by adding terms. 
The first term we add is  $- \sum\limits_{e \in \edgesi}\myIntS e { \jump{ u \average{ A \nabla v}}}$ which equals zero when evaluating the integral for a smooth $v$. Hence it will vanish for the solution of the generalised Poisson problem. The second term required to achieve symmetry is  $-\sum\limits_{e \in \edgesb}\myIntS e { u A \nabla v \cdot \mathbf n}$. Since the Dirichlet boundary condition gives us solution values at the boundary this term equals $\sum\limits_{e \in \edgesb}\myIntS e { g A \nabla v \cdot \mathbf n}$. Thus, we simply add it to both to the bilinear form and to the right-hand side functional.
\begin{align}
 a_S(v, u) = &\sum\limits_{T \in \triang} \myIntX  T { \nabla v \cdot A \nabla u} \nonumber \\
  &-\sum\limits_{e \in \edgesi}\myIntS e { \jump {v \average{A \nabla u} }}
 - \sum\limits_{e \in \edgesi}\myIntS e { \jump{ u \average{ A \nabla v}}} \nonumber\\ 
 & - \sum\limits_{e \in \edgesb}\myIntS e { v A \nabla u \cdot \mathbf n} 
    - \sum\limits_{e \in \edgesb}\myIntS e { u A \nabla v \cdot \mathbf n} \nonumber
\end{align}
and 
\begin{align}
	l(v) =& \sum\limits_{T \in \triang} \myIntX  T { v f}
		 -\sum\limits_{e \in \edgesb}\myIntS e { g A \nabla v \cdot \mathbf n}.
\end{align} 
Since we defined the jump and average also on boundary edges (cf. Definition \ref{def: edge operators}) we can shorten the term to 
\begin{align}
a_S(v, u)= &\sum\limits_{T \in \triang} \myIntX  T { \nabla v \cdot A \nabla u} \nonumber \\  &-\sum\limits_{e \in \edges}\myIntS e { \jump {v \average{A \nabla u} }}
 - \sum\limits_{e \in \edges}\myIntS e { \jump{ u \average{ A \nabla v}}}. \label{eq:inner product SIPG}
\end{align}

%and $f$
%\begin{align}
%	f_S(v,v) = && \sum\limits_{T \in \triang} \myIntX  T { v f \\
%	 				&+ &\sum\limits_{e \in \edgesb}\myIntS e { v \cof(D^2 w) \nabla u \cdot n \\
% &+ &\sum\limits_{e \in \edgesi}\myIntS e { u \llbracket \cof(D^2 w) \nabla v \cdot n\rrbracket \\
%	&-  &\sum\limits_{T \in \triang} \myIntX  T { u (\nabla \cdot \cof(D^2w)) \cdot \nabla v \\
%\end{align} 
We need to enforce continuity and compliance with the boundary conditions as our ansatz space is discontinuous and allows arbitrary boundary values. To do that, we penalise the jump across interior and boundary edges as described in  \cite[3.2.2.]{PPO+2000} by adding the two following terms to $a_S$ and $l$, respectively
\begin{align}
	J^\sigma(v, u) = \sum\limits_{e \in \edgesi} \myIntS e { \frac \sigma {|e|} \jump v \jump u}
		 \;\; \textnormal{ and } \;	\;
		 J^\sigma_0(v) = \sum\limits_{e \in \edgesb} \myIntS e {\frac \sigma {|e|} v g} . \label{eq: penalty term}
\end{align}

Thus, we end up with the problem of finding $v \in V_h$ such that
\begin{align}
	a_h^{DG}(v, u) = l^{DG}_h (v) \qquad \forall v \in V \label{eq: DG system}
\end{align}
where
\[ 	
	a_h^{DG} (v, u) := a_S(v,u) + J^\sigma(v,u) \text{ and } l_h^{DG}(v) := l(v) + J^\sigma_0(v).
\]

%The authors of \cite{PPO+2000} suggest to choose the penalty term $\sigma$ as $10 k^2$ and undergird that 

Just like in the finite element method we can derive a linear system of equations $M \mathbf{u} = \mathbf{l}$ with $M$ sparse and $\mathbf{u}$ representing the coefficient vector of $u_h$ with respect to the basis $B_V$.
This particular discontinuous Galerkin method is called \emph{Symmetric Interior Penalty Galerkin method} (SIPG).

\input{wellposedness_DG.tex}

\input{implementationDG.tex}

After we saw the main implementation points of a DG method we need some algebraic and analytic identities for the Hessian matrix to develop a DG method handling the \MA equation.
\section{Hessian Identities}

We begin with the notion of the cofactor matrix which is later important for the linearisation of the \MA equation.
\begin{definition}[Cofactor Matrix] \label{def: cof matrix}
	The \emph{cofactor matrix} of a matrix $A \in \R^{d \times d}$ is defined by the entries
	\begin{align}
	(\mycof A )_{i,j} := (-1)^{i+j} \mydet{A_{ij}},
	\end{align}
	where $A_{ij}$ denotes the matrix resulting from deleting the $i$-th row and $j$-column in $A$.
\end{definition}

We observe that in the two-dimensional case the cofactor matrix of the Hessian simplifies to
\begin{align}
\mycof {D^2 u} = \begin{pmatrix}
								\dxx{x_2} u & -\frac {\partial }{\partial x_1 x_2} u\\
								-\frac {\partial }{\partial x_2 x_1} u & \dxx{x_1} u
							\end{pmatrix}.
\end{align}

\begin{definition}[Frobenius Product] \label{def: frobenius product}
	The \emph{Frobenius product} : of two matrices $A, B \in \R^{d \times d}$ is defined by
	\[
		A:B := \sum_{i,j=1} ^d a_{i,j} b_{i,j}.
	\]
\end{definition}

Very helpful is the relation between the determinant and the cofactor matrix.
\begin{lemma}\label{la: rel det cofactor}
	For every matrix $A  \in \R^{d \times d}$ it holds
	\[
		d \mydet A = \mycof A: A.
	\]
\end{lemma}
\begin{proof}
	By Laplace's formula it holds for any $1 \leq j \leq d$
	\[
		\mydet A = \sum\limits_{i = 1}^{d} a_{i,j} (\mycof A )_{i,j}.
	\]
	Summing over all $j$ yields
	\[
		d \mydet A = \sum\limits_{j= 1}^{d} \sum\limits_{i = 1}^{d} (\mycof A )_{i,j}  a_{i,j}  = \mycof A: A.
	\]
	\phantom{blub}
\end{proof}

Another advantage is the behaviour of the cofactor matrix of a Hessian if a divergence is applied to it:
\begin{definition}[Matrix Divergence]
The divergence $\nabla \cdot A$ for a matrix $A$ is defined by taking the divergence row-wise, i.e.
\[
	\nabla \cdot A := \begin{pmatrix} \nabla \cdot A_1 \\ \vdots \\ \nabla \cdot A_d\end{pmatrix}
	= \begin{pmatrix} \sum_{j = 1}^{d} \dx{x_j} a_{1,j} \\ \vdots \\ \sum_{j = 1}^{d} \dx{x_j} a_{d,j}\end{pmatrix}.
\]
where $A_i$ denotes the $i$-th row of $A$.
	
\end{definition}

\begin{lemma}[Divergence-Free Property of Cofactor Matrices] \label{la: divergence free cof}
For smooth functions $u:\Omega \rightarrow \R$ with $\Omega \subset \R^2$ the cofactor matrix of the Hessian is divergence-free, i.e.
\[
	\nabla \cdot \mycof{D^2 u} = 0.
\] 
\end{lemma}
\begin{proof}
It holds that
\begin{align*}
	\nabla \cdot \mycof{D^2 u} = \sum_{i=1}^{2} \dx{ x_i}\mycof{D^2 u}_i = 
	\begin{pmatrix}
		\frac {\partial^3} {\partial x_1 {x_2}^2 } u -\frac {\partial^3} {\partial{x_2} x_1 {x_2}} u\\
				\frac {\partial^3} {\partial {x_1}x_2 {x_1}} u-\frac {\partial^3} {\partial x_2 {x_1}^2 } u
	\end{pmatrix}.
\end{align*}
By Schwarz' theorem the latter equals zero if $u$ is twice continuous differentiable.
\end{proof}

The divergence-free property yields to an integration by parts rule.
\begin{lemma}[Integration by parts for the Frobenius product] \label{la: integration by parts Frobenius}
For a Frobenius product with the Hessian's cofactor matrix  the following integration by parts rule holds for smooth $u$ and any $B \in [H^1(\Omega)]^{2 \times 2}$
\[
	\myIntX  \Omega { (D^2 u : B)} = 
		- \myIntX  \Omega { (\nabla \cdot B) \cdot \nabla u }
		+ \myIntS  {\partial \Omega} {  B \nabla u \;\bf n}.
\] 
\end{lemma}

\begin{proof}
The proof is based on applying integration by parts row-wise. Hence, we have
\begin{align*}
- \myIntX  \Omega { (\nabla \cdot B) \cdot \nabla u} &= 
- \myIntX  \Omega { \sum_{i = 1}^{d} (\nabla \cdot B_i) \dx {x_i} u} \\
&=  \myIntX  \Omega { \sum_{i = 1}^{d} B_i \cdot  (\nabla \dx {x_i} u)} 
	- \myIntS  {\partial \Omega} { \sum_{i = 1}^{d} (B_i) \dx {x_i} u \;\mathbf{n}} \\
&=  \myIntX  \Omega { \sum_{i = 1}^{d}\sum_{j= 1}^{d} B_{ij} \dxy {x_i}{x_j} u}
	- \myIntS  {\partial \Omega} { B \nabla u \;\mathbf{n}} \\
&=  \myIntX  \Omega { (D^2 u : B)}
	- \myIntS  {\partial \Omega} { B \nabla u \;\mathbf{n} }.
\end{align*}
\end{proof}

In future talking about weak formulations it is handy to have a divergence form of the latter Frobenius product.
\begin{lemma}[Divergence form of the Frobenius Product] \label{la: An application of the divergernce product rule}
For smooth $u:\Omega \rightarrow \R$ with $\Omega \subset \R$ and $v\in H^2(\Omega)$, there holds
\[
		\nabla \cdot \left( \mycof {D^2 u } \nabla v \right) %- \nabla \cdot \left(\mycof {D^2 u }\right) \nabla v
		= \mycof {D^2 u}: D^2 v.
\] 
\end{lemma}

\begin{proof}
By the definition of the divergence and the product rule we obtain
\begin{align*}
\nabla \cdot \left( \mycof {D^2 u } \nabla v \right) =&%- \nabla \cdot \left(\mycof {D^2 u }\right) \nabla =& 
\sum_{i= 1}^{d} \dx {x_i} 	\left( \sum_{j= 1}^{d} \mycof {D^2 u }_{i,j} \dx{x_j} v \right)\\
%&-  \sum_{j= 1}^{d}  \sum_{i= 1}^{d} \left(\dx {x_i} \mycof {D^2 u }_{i,j}  \right) \dx{x_j} v \\
=&  \sum_{i= 1}^{d} \sum_{j= 1}^{d}  \left(\dx {x_i} \mycof {D^2 u }_{i,j}  \right) \dx{x_j}v + \sum_{i= 1}^{d} \sum_{j= 1}^{d}  \mycof {D^2 u }_{i,j} \dxy{x_j}{x_i}v\\
=&  \sum_{j= 1}^{d}  \nabla \cdot \left(\mycof {D^2 u }\right)_j \dx{x_j}v + \sum_{i= 1}^{d} \sum_{j= 1}^{d}  \mycof {D^2 u }_{i,j} \dxy{x_j}{x_i}v\\
=&   \nabla \cdot \left(\mycof {D^2 u }\right) \nabla v+ \mycof {D^2 u }:D^2v.
\end{align*}
The statement results from the fact that the cofactor matrix of the Hessian is divergence-free.
\end{proof}
