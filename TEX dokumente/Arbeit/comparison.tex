\section{Comparison with Newton's method}

Before we start the anlaysis of our Method we shortly see what happens if we apply Newton's method on the analytical form of the \MA equation \ref{MA eq}.
Let $F$ be the function such that its root solves the \MA equation, i.e. 
\[
	F(u) = \mydet{D^2 u} -f
\]
Applying Newton's method on $F(u) =0$ we have
\begin{align}
	DF[u^n](u^{i+1}-u^i) = -F[u^i]
\end{align}
where $DF[u]$ denotes the G\^ateaux derivative. We derived the G\^ateaux derivative $DF[u] v = \mycof{D^2 u}:D^2v$ already in theorem \ref{thm: linearisation} leading us to the Newton iteration
\begin{align}
	\mycof{D^2 u^i}:D^2\left(u^{i+1}-u^i\right) &= -\mydet{D^2 u^i}+f \nonumber \\
	\Leftrightarrow \qquad \qquad  \mycof{D^2 u^i}:D^2(u^{i+1}) &= -\mydet{D^2 u^i} +f  +\mycof{D^2 u^i}:D^2(u^i). \label{eq: Newton iteration pre}
\end{align}

Similar to the derivation of the fixed point iteration we can apply Lemma \ref{la: An application of the divergernce product rule} to rewrite \eqref{eq: Newton iteration pre} and we have the problem
\begin{align}
	\nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i+1} \right) &= -\mydet {D^2u^i} +f+\nabla \cdot \left( \cof(D^2 u^i) \nabla u^{i} \right)  \textnormal{ in } \Omega,  \label{eq: Newton iteration}\\
	u^{i+1} &= g \textnormal{ on } \partial \Omega .
\end{align}

Hence, considering once again the fact 
\[
\nabla \cdot \left( \mycof {D^2 u } \nabla v \right)
\stackrel{La.\ref{la: An application of the divergernce product rule}}=\nabla \cdot {}\mycof{D^2 u}:D^2u
=\frac 1 2 \mydet{D^2u}.
\]
we can see our method as a variant of Newton's method. 

In \cite{Awanou2014} Awanou analysed an iteration process similar to ours:
\begin{align}
	\nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i+1} \right) &= \nabla \cdot \left( \cof(D^2 u^0) \nabla u^{i} \right) + f - \operatorname{det} (D^2u^i) \textnormal{ in } \Omega,  \label{eq: Awanout eq}\\
	u^{i+1} &= g \textnormal{ on } \partial \Omega.
\end{align}
He proves convergence for the analytical solution $u$ and a sufficent close $u^0$. 
In a earlier work \cite{Awanou2010} Awanou examined a discrete Version of a vanishing moment method, herein he mentioned a method he calls Newton's method: Find $u_h^{i+1}\in V_h$ such that for all $v_h \in V_h \cap H^1_0 (\Omega)$
\[
	\myIntX {\Omega} {[\mycof{ D^2 u_h^i} Du_h^{i+1}] \cdot Dv_h} 
	= -	\myIntX{\Omega} {f v_h} 
		+ \frac 1 2 \myIntX{\Omega} {[\mycof{ D^2 u_h^i} Du_h^{i}] \cdot Dv_h} \;  \label{eq: Awanout eq2}.
\]
And indeed this is the variational form of \eqref{eq: Newton iteration}
His chosen trial space were piecewise polynomials contained in $C^1(\Omega)$. He claims that this ansatz breaks down for problems with non-smooth solutions, in his numerical results he cites test \ref{test sqrt} as an example where Newton's method diverges.

\todo{existence theory}

%\begin{align}
%	&\myInt_\Omega \cofHess {u^i} \nabla (u^{i+1}-u^i) \nabla v - \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla (u^{i+1}-u^i) \cdot \mathbf n \; v \nonumber \\
%	&- \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla v \cdot \mathbf n (u^{i+1}-u^i) + \sigma \sum_{e \in \edgesb} \frac 1 {|e|} \myIntS e { v (u^{i+1}-u^i)\nonumber \\
%	=&-\myInt_\Omega \left(f - \detHess{u^i)}\right) v  
%			- \sum_{e \in \edgesi} \myIntS e { \jump { \average{\cofHess {u^i}} \nabla {u^i} } v \nonumber \\
%&	- \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla v \cdot \mathbf n \; (u^i-g) - \sigma \sum_{e \in \edgesb} \frac 1 {|e|} \myIntS e { v (u^{i}-g) \label{eq: a newton step Brenner}
%\end{align}
%To compare both linearisations we first reorder and remove cancelling terms in \eqref{eq: a newton step Brenner}.
%\begin{align}
%	&\myInt_\Omega \cofHess {u^i} \nabla u^{i+1} \nabla v \\
%	&- \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla u^{i+1} \cdot \mathbf n \; v 
%		- \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla v \cdot \mathbf n \; u^{i+1} \\
%	&+\sigma \sum_{e \in \edgesb} \frac 1 {|e|} \myIntS e { v u^{i+1}\\
%	=
%	&-\myInt_\Omega \left(f - \detHess{u^i)}\right) v \\
%	&- \sum_{e \in \edgesi} \myIntS e { \jump { \average{\cofHess {u^i}} \nabla {u^i} } v 
%		+ \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla {u^i} \cdot \mathbf n \; v\\
%	&- 2\sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla v \cdot \mathbf n \; {u^i}
%		+ \sum_{e \in \edgesb} \myIntS e { \cofHess {u^i} \nabla v \cdot \mathbf n \; g \\
%	&+\sigma \sum_{e \in \edgesb} \frac 1 {|e|} \myIntS e { v g \label{eq: ordered newton step Brenner}
%\end{align}

\section{Analysis of the DG Method} \label{sec: DG analysis}

\subsection{Comparison with a $C^0$ Penalty method}
It is also interesting to compare our linearisation with the linearisations of the two other methods.
Consider again the linearisation of the Finite Element method mentioned in Section \ref{sec: Brenner method}. The authors develop the linear mapping $L^B_u:H^2(\Omega; \triang) \rightarrow [H^2(\Omega; \triang) \cap H^1(\Omega)]'$ given by
\begin{align}
\bilin {L^B_{u} w} v
	=&\myIntX  \Omega { (\cofHess{ u}\nabla w) \nabla v}
		- \sum_{e \in \edgesb} \myIntS e { (\cofHess{u}\nabla w) \cdot \mathbf n \; v} \nonumber \\
		&-  \sum_{e \in \edgesb} \myIntS e { (\cofHess{u}\nabla w) \cdot \mathbf n \; v} 
		+ \sigma \sum_{e \in \edgesb} \frac 1 {|e|} \myIntS e { v w}.
		\label{eq: linearisation brenner}
\end{align}
They say it is important that this linearisation is a consistent and stable discretatisation of the linearisation of the \MA equation, namely $\nabla \cdot \left( \mycof {D^2 u } \nabla (v) \right)$.

Looking now at our Picard iteration given by \ref{eq:fixed point iteration} we see that we aim to solve the discretised linearisation of the \MA equation at the point $u^i$ in every step.
Let $V$ be $H^2(\Omega; \triang) \cap L^2(\Omega)$ and $L_u: V \rightarrow V'$ denote the linear mapping describing the left-hand side of the SIPG method to perform an iteration step, i.e.
\begin{align}
	\bilin {L_u w} v =
 &\myIntX  \Omega { \nabla v \cdot \cof(D_h^2 u) \nabla w } \nonumber\\
 & -\sum\limits_{e \in \edges}\myIntS e { \jump {v \average { \cof(D_h^2 u^{i}) \nabla w} }}
 - \sum\limits_{e \in \edges}\myIntS e { \jump {w \average{ \cof(D^2 u) \nabla v} }} \nonumber\\  
 & +\sum\limits_{e \in \bigEps} \myIntS e { \frac \sigma {|e|} \jump {v}  \jump {w}}. \label{eq: linearisation our}
\end{align}
And indeed comparing \eqref{eq: linearisation our} with \eqref{eq: linearisation brenner} we notice that they are equal except for the interior edge integrals. But this difference is only due to the different choice of ansatz spaces: While Brenner et alt. use $C^0$ elements the Picard iteration bases on $DG$ elements.

Consider $u\in H^s(\Omega)$ for $s>3$ to be the convex solution of the \MA problem. Brenner et alt. formulated a nonlinear finite element method with the goal that its linearisation at the solution $u$ is given by \eqref{eq: linearisation brenner}. In other words their penalty method could be formulated by a mapping $F^B:H^3(\Omega; \triang) \rightarrow [H^2(\Omega; \triang) \cap H^1(\Omega)]'$ satisfying
 \begin{align}
 	F^B(u +w ) = L^B_u w + R^Bw \label{eq: add brenner method}
 \end{align}
 with a mapping $R^B:H^3(\Omega; \triang) \rightarrow [H^2(\Omega; \triang) \cap H^1(\Omega)]'$. 
% They compute $R^B$ to be given by
% \begin{align}
% 	\bilin {R^B w} v 
% 	= & - \myInt_\Omega \detHessH{w} v dx 
% 		+ \sum_{e \in \edgesi} \jump {\average{\cofHessH{w}} \nabla w} v ds \nonumber \\
% 		&- \sum_{e\in \edgesb} \myIntS e { \cofHessH{w} \nabla v \cdot \mathbf{n} \; w ds 
% \end{align}.
% 
The goal is to prove \eqref{eq: fixed point functional} has a solution in $V_h$. To this end, we consider for the rest of the analysis a convex and polyhedral domain $\Omega\in \R^2$, a positive $f \in C^{k+1}$ and $g\in C^{k+3}(\partial \Omega)$. By the results of \cite{CNS1984} the \MA problem then has a convex solution $u$. Note, that $u$ denotes from now on a fixed function.
We try to mimic the argumentative structure Brenner et alt. used to show convergence of their method. At first, we introduce the functional $R_u \in V'$ for the right-hand side of the SIPG method defined by
\[
\bilin {-R_u} v = - 2 \myIntX  \Omega {v f}
-\sum\limits_{e \in \edgesb}\myIntS e { g \cofHessH{u} \nabla v \cdot n }
+\sum\limits_{e \in \edgesb} \myIntS e { \frac \sigma {|e|} v g} \label{eq: definition R}
\]
Our desired Picard iteration is then formulated by: Find $u^{i+1}_h \in V_h$ such that
\begin{align}
\bilin {L_{u^i_h} w_h} v =  \bilin {-R_{u^i_h}} v   \qquad \forall v \in V. \label{eq: fixed point functional}
\end{align}
Getting everything on one side the iteration reads: Find $u_h^{i+1}V_h$ being the root of the function $F:V \rightarrow V'$ defined by
\begin{align*}
 {Fw} =  L_{u^i_h} w + R_{u^i_h}  \qquad \text{ for } w \in V
\end{align*}

Thus we have the identity 
\begin{align}
	F(u+w) = L_{u^i_h} (u+w) + R_{u^i_h} = L_{u^i_h} u +L_{u^i_h}w + R_{u^i_h} = Fu + L_{u^i_h}w. \label{eq: add our method}
\end{align}

Let us compare \eqref{eq: add brenner method} with \eqref{eq: add our method}: We have already the seen the commonality of both methods, they base on the same linearisation and thereby try to recover the permutability between linearisation and discretisation. 
But there are also two major differences: The one is that the right-hand side of \eqref{eq: add brenner method} contains a nonlinear mapping $R^B$. The other is that the second formulation contains the term $Fu$. \\
Note, that the first equation could also be written as 
 \begin{align*}
 F^B(u +w ) = F^B u + L^B_u w + R^Bw 
 \end{align*}
since the exact \MA solution is a root of the finite Element method. 

These differences also characterises the difference between the two approaches. 
While the first reproduces the nonlinearity of the \MA equation with a nonlinear term, the second fixed parts of the equation and traded nonlinearity for consistency errors.

\subsection{Consistency Estimates}

Let for $w \in V$ the mapping $L_{w,h}:V_h \rightarrow V_h'$ be the restriction from $L_w$ to $V_h$. Then all further analysis bases on the following lemma
\begin{lemma}[Stability] \label{la: stability L}
	Suppose $u_h^i \in V_h$ to be convex. Then 
	\begin{align}
		\HMinusOneDnorm{L_{u_h^i}v} \leq C \eNorm{v} \qquad \forall v \in V
	\end{align}
	Further for $v_h \in V_h$ and a penalty paramter $\sigma $ sufficiently large $L_{u^i_h,h}: V_h \rightarrow V_h'$ is invertible and satisfies
	\begin{align}
		\eNorm{L_{u^i_h,h}^{-1} r_h} \leq C \HMinusOneDnorm{r_h} \qquad \forall r_h \in V_h'
	\end{align}
\end{lemma}
\begin{proof}
	Since for convex $u_h^i$ the cofactor matrix is always positive definite the claim follows from Theorem \ref{thm: SIPG stability}.
\end{proof}

For further analysis we want to examine the dependence of $L_w$ on the parameter $w$.
\begin{lemma}\label{la: L dependence paramter}
	For $w_1, w_2 \in V$ holds
	\[
		\HMinusOneDnorm{L_{w_1} v - L_{w_2} v} \leq \epsilonTwo{w_1 - w_2} \eNorm{v} \qquad \forall v \in V,
	\]
	where $\epsilonTwoZeroArg:V \rightarrow \R$ is defined by
	\[
		\epsilonTwo{v} = \max{ \LTwonorm{\hess v}}{ \sup_{e \in \edges} \LTwonormE{\average{\hess v}}}
	\]	
\end{lemma}
\begin{proof}
	It holds for a positive definite matrix $A$ that 
	\begin{align*}
		\LTwonorm{A} =  \sup_{v \in L^2(\Omega), \LTwonorm{v}=1} \myIntX \Omega {v^t A^t A v} = \lambda^2
	\end{align*}
	where $\lambda$ is the largest eigenvalue of $A$. Since for every $w \in V$ the eigenvalues of $\cofHessH{w}$ and $\hess{w}$ coincide on every $T \in \triang$ we obtain 
	\begin{align}
		\LTwonormT{\cofHessH{w}} = \LTwonormT{\hess{w}} \qquad \forall T \in \triang.\label{eq: equality norm}
	\end{align}
	Furthermore holds for any positive definite matrices $A,B$ that the sum of their eigenvalues are the eigenvalues of $A+B$. Thus, we find
	\begin{align}
		\LTwonormE{\average {\cofHessH{w}}} = \LTwonormE{\average {\hess{w}}} \qquad \forall e \in \edges. \label{eq: equality norm average}
	\end{align}
	Let $w_1, w_2, v$ be in $V$. For any $\varphi_h \in V_h$ we have by the definition of $L$ (cf. \eqref{eq: linearisation our}
	\begin{align}
		\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} =  
			&\myIntX  \Omega { \nabla \varphi_h \cdot \cofHessH{(w_1-w_2)} \nabla v}  \nonumber\\
			& -\sum\limits_{e \in \edges}\myIntS e { \jump {\varphi_h \average { \cofHessH{(w_1-w_2)} \nabla v} }} \nonumber \\
			& - \sum\limits_{e \in \edges}\myIntS e { \jump {v \average{ \cofHessH{(w_1-w_2)} \nabla \varphi_h} }} \label{eq: diff linear}
	\end{align}
	As already done in \eqref{eq: CS estimate} we use once the Cauchy-Schwarz inequality and once the discrete Cauchy-Schwarz inequality on every summand and obtain
	\begin{align*}
		\eqref{eq: diff linear}%\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} 
		\leq& \LTwonorm{\nabla \varphi_h} \LTwonorm{\cofHessH{(w_1 -w_2)}} \LTwonorm {\nabla v} \nonumber \\
			&+ \left( \sum_{e \in \edges} \frac {\sigma}{|e|}\LTwonormE{\jump {\varphi_h}}^2 \right)^{\frac 1 2}
			   \left( \sum_{e \in \edges} \frac {|e|} \sigma\LTwonormE{\average{\cofHessH{(w_1-w_2} \nabla v}}^2 \right)^{\frac 1 2} \\
			&+ \left( \sum_{e \in \edges} \frac {\sigma}{|e|}\LTwonormE{\jump v}^2 \right)^{\frac 1 2}
			\left( \sum_{e \in \edges} \frac {|e|} \sigma\LTwonormE{\average{\cofHessH{(w_1-w_2} \nabla \varphi_h}}^2 \right)^{\frac 1 2}
	\end{align*}
	and hence with \eqref{eq: equality norm} and \eqref{eq: equality norm average}
	\begin{align*}
		\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h}
		\leq& \epsilonTwo{w_1 - w_2} 
			\left( \LTwonorm{\nabla \varphi_h} \LTwonorm {\nabla v} 
				\phantom{\left( \sum_{e \in \edges} \frac {|e|} \sigma\LTwonormE{\average{\nabla \varphi_h}}^2 \right)^{\frac 1 2}}
			 \right.\nonumber \\
			&\phantom{\epsilonTwo{w_1 - w_2}}+ \left( \sum_{e \in \edges} \frac {\sigma}{|e|}\LTwonormE{\jump {\varphi_h}}^2 \right)^{\frac 1 2}
			\left( \sum_{e \in \edges} \frac {|e|} \sigma\LTwonormE{\average{\nabla v}}^2 \right)^{\frac 1 2} \\
			&\phantom {\epsilonTwo{w_1 - w_2}}+ \left( \sum_{e \in \edges} \frac {\sigma}{|e|}\LTwonormE{\jump v}^2 \right)^{\frac 1 2}
			\left. \left( \sum_{e \in \edges} \frac {|e|} \sigma\LTwonormE{\average{\nabla \varphi_h}}^2 \right)^{\frac 1 2} \right).
	\end{align*}
	Using now the discrete Cauchy-Schwarz inequality on the whole term yields
	\begin{align*}
		\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} 
		\leq& \epsilonTwo{w_1 - w_2}
			\left( 
				\LTwonorm{\nabla \varphi}^2
					+ \sum\limits_{e \in \edges} \frac {\sigma} {|e|}\LTwonormE{\jump {\varphi}}^2
					+ \sum\limits_{e \in \edges} \frac {|e|} \sigma \LTwonormE{\average{\nabla \varphi}}^2
				\right)^{\frac 1 2} \nonumber \\
			&\times
			\left( 
				\LTwonorm{\nabla v}^2
					+\sum\limits_{e \in \edges} \frac {\sigma}{|e|}\LTwonormE{\jump {v}}^2
					+ \sum\limits_{e \in \edges} \frac {|e|} \sigma \LTwonormE{\average{\nabla v}}^2
			\right) ^{\frac 1 2} \nonumber \\
			\leq & \epsilonTwo{w_1 - w_2} \eNorm \varphi \eNorm v.
	\end{align*}
	The result follows then with Definition \ref{def: h-1 seminorm}.
%	Since the eigenvalues of $\cofHessH{(w_1-w_2}$ and $\hess{(w_1-w_2)}$ coincide their $L^2$ norm also coincides. Hence, by this and the discrete Cauchy-Schwarz inequality it follows
%	\begin{align}
%		\eqref{eq: diff linear 2} %\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} 
%		\leq& \LTwonorm{\nabla \varphi_h} \LTwonorm{\hess{(w_1 -w_2)}} \LTwonorm {\nabla v} \nonumber \\
%			&+ \LInftynorm{\jump {\varphi_h}} 
%				\left(\sum_{e \in \edges} \LTwonormE{\average{\hess{(w_1-w_2}}}^2 \right)^{\frac 1 2} 
%				\left(\sum_{e \in \edges} \LTwonormE{\average{\nabla v}}^2 \right)^{\frac 1 2} \nonumber \\
%			&+ 	\left(\sum_{e \in \edges} \LTwonormE{\jump v}^2 \right)^{\frac 1 2} 
%				\left(\sum_{e \in \edges} \LTwonormE{\average{\hess{(w_1-w_2}}}^2 \right)^{\frac 1 2} 
%				\LInftynorm{\average{\nabla \varphi_h}}. \label{eq: estimate diff L}
%	\end{align}
%	By the definition of jump and average it is clear that 
%	\[
%		\LInftynorm{\jump {\varphi_h}} = \LInftynorm{\varphi_h}\text{ and }\LInftynorm{\average{\nabla \varphi_h}}= \LInftynorm{\nabla \varphi_h}
%	\]
%	hold. Further we can estimate the $L^\infty$ norm using the discrete Sobolev inequality (\todo{discrete sobolev}) and we find
%	\begin{align}
%		\LInftynorm{\average {\nabla \varphi_h}} \leq C (1+|\ln h|^{\frac 1 2}) \LTwonorm{\nabla \varphi_h} \leq C (1+|\ln h|^{\frac 1 2}) \eNorm{\nabla \varphi_h}, \label{eq: estimate average phi}
%	\end{align}
%	and
%	\begin{align}
%		\LInftynorm{\jump {\varphi_h}} \leq C (1+|\ln h|^{\frac 1 2}) \LTwonorm{\varphi_h}. \label{eq: first estimate jump phi}
%	\end{align}
%	To further estimate \eqref{eq: first estimate jump phi} we apply the inverse estimate from Lemma \ref{la: inverse estimate} and the Poincar\'e inequality (\todo{poincare inequality}) and obtain
%	\begin{align}
%		\LTwonorm{\varphi_h} = \sum_{T \in \triang} \LTwonormT{\varphi_h} \leq C h \sum_{T \in \triang} \HOnenormT{\varphi_h} \leq C h \sum_{T \in \triang} \singleNorm{\varphi_h}_{H^1(T)} \leq Ch \eNorm{\varphi_h} \label{eq: estimate phi}
%	\end{align}
%	By \eqref{eq: estimate average phi}, \eqref{eq: first estimate jump phi} and \eqref{eq: estimate phi} and $h<1$, we have
%	\begin{align}
%		\maxTriple{\LTwonorm{\nabla \varphi}}{\LInftynorm{\jump{\varphi_h}}}{\LInftynorm{\average{\nabla \varphi_h}}} \leq C (1+|\ln h|^{\frac 1 2}) \eNorm{\varphi_h}
%	\end{align} 
%	and hence by \eqref{eq: estimate diff L}
%	\begin{align}
%		\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} 
%		\leq& C (1+|\ln h|^{\frac 1 2})\eNorm{\varphi} 
%			\left[ 
%				\LTwonorm{\hess{(w_1 -w_2)}} \LTwonorm {\nabla v} \nonumber \right.\\
%				&+ 	\left(\sum_{e \in \edges} \LTwonormE{\average{\hess{(w_1-w_2}}}^2 \right)^{\frac 1 2} 
%					\left(\sum_{e \in \edges} \LTwonormE{\average{\nabla v}}^2 \right)^{\frac 1 2} \nonumber \\
%				&+ 	\left(\sum_{e \in \edges} \LTwonormE{\jump v}^2 \right)^{\frac 1 2} 
%				\left. \left(\sum_{e \in \edges} \LTwonormE{\average{\hess{(w_1-w_2}}}^2 \right)^{\frac 1 2} 
%			\right] \label{eq: difference estimate number two}
%	\end{align}
%\todo{brackets}	
%	Using the discrete Cauchy-Schwarz inequality we find
%	\begin{align}
%	\bilin{L_{w_1}v- L_{w_2} v} {\varphi_h} 
%		\leq& C (1+|\ln h|^{\frac 1 2})\eNorm{\varphi} 
%		\nonumber \\
%		&\times
%			\left(
%				\LTwonorm{\hess{(w_1 -w_2)}}^2 
%				+ 2\sum_{e \in \edges} \LTwonormE{\average{\hess{(w_1-w_2}}}^2
%			\right)^{\frac 1 2} \nonumber \\
%	   	&\times
%			\left(
%			\LTwonorm{\nabla v}^2 
%			+ \sum_{e \in \edges} \frac {|e|}\sigma \LTwonormE{\average{\nabla v}}^2
%			+\sum_{e \in \edges} \frac \sigma {|e|}\LTwonormE{\jump {v}}^2
%			\right)^{\frac 1 2} \nonumber \\
%		\leq& C (1+|\ln h|^{\frac 1 2})\eNorm{\varphi} 	\epsilonTwo{{(w_1-w_2)}} \eNorm{v}
%	\end{align}
% \phantom{blub}
\end{proof}

No we are able to establish an estimate of the consistency error of $Fu$.
\begin{theorem}[Consistency Error of $F$] \label{la: consistency error F}
	There holds
	\begin{align*}
	\HMinusOneDnorm{Fu} \leq& C \epsilonTwo{u-u_h^i}
	\end{align*}
	where $C$ is a positive constant $C$ that depends on the mesh and $u^i_h$, but not on $h$ and $u$. 
\end{theorem}
\begin{proof}
	We have for $v \in V_h$
	\begin{align}
	%		\bilin{Fu} v = \myIntX  \Omega {
	\HMinusOneDnorm{Fu} =& \HMinusOneDnorm{ L_{u^i_h}u + R_{u^i_h}} \nonumber\\
	\leq& \HMinusOneDnorm{ L_{u^i_h}u - L_u u} + \HMinusOneDnorm{ L_{u}u + R_u} + \HMinusOneDnorm{ -R_u + R_{u^i_h}} \label{eq: Fu first estimate}
	\end{align}	
	The exact solution $u$ is a fixed point of \eqref{eq: variational form}, and the mappings $L_w$ and $R_w$ were defined to formulate the SIPG method for this equation fixing the $u$ in the cofactor matrix. Thus, we obtain that 
	\begin{align}
	L_{u} u + R_u = 0. \label{eq: right solution L+U}
	\end{align}
	For any $v \in V_h$ it holds by the definition of $R$ (cf. \eqref{eq: definition R}) and the Cauchy-Schwarz inequality
	\begin{align}
	\bilin {R_u - R_{u^i_h}} v 
	=& \sum_{e \in \edgesb} \myIntX \Omega {g \cofHessH{(u-u_h^i)} \nabla v \cdot \mathbf n }\nonumber \\
	\leq& \LInftynormPOm{g}
		\sum_{e \in \edgesb} \LTwonormE{ \cofHessH{(u-u_h^i)} \nabla v }^2 \nonumber \\
	\leq& \LInftynormPOm{g}
		\left( \sum_{e \in \edgesb} \frac {|e|} \sigma \LTwonormE{ \cofHessH{(u-u_h^i)}}^2  \right)^{\frac 1 2}
		\left( \sum_{e \in \edgesb} \frac \sigma {|e|} \LTwonormE{\nabla v }^2  \right)^{\frac 1 2}	 \label{eq: R first estimate}
	\end{align}
	Using again the fact from \eqref{eq: equality norm}, namely that the $L^2$ norm of a matrix and its cofactor matrix conicide, as well as the standard combination of the trace theorem from Lemma \ref{la: trace estimate} and the inverse estimate from Lemma \ref{la: inverse estimate} we find
	\begin{align}
	\sum_{e \in \edgesb} \frac 1 {|e|} \LTwonormE{ \cofHessH{(u-u_h^i)}}^2 
	\leq C \sum_{T \in \triang} \LTwonormT{\hess{(u-u_h^i)}}^2.
	\end{align}
	Combining this with \eqref{eq: R first estimate} yields
	\begin{align*}
	\bilin {R_u - R_{u^i_h}} v \leq C \eNorm{v} \epsilonTwo{u-u_h^i}
	\end{align*}
	and hence 
	\begin{align}
	\HMinusOneDnorm{R_u - R_{u^i_h}} \leq C \epsilonTwo{u-u_h^i}. \label{eq: contraction R}
	\end{align}
	The consistency error estimate then follows from \eqref{eq: Fu first estimate}, Lemma \ref{la: L dependence paramter}, \eqref{eq: right solution L+U} and \eqref{eq: contraction R}.
\end{proof}

This result is very important to understand the performances of the derived method. It states the consistency error depends proportionally on the error $\epsilonTwo{u-u^i_h}$. That means the method is only consistent if $\epsilonTwo{u-u^i_h}$ depends proportionally on $h^l$ for a $l \geq 1$. If not the consistency error made is not controllable and in particular the introduced method is not consistent.

\subsection{Error estimates}
Now we want to transfer the argument structure given by Brenner et alt. We first define the operator $\mathcal M: V \rightarrow V_h$
\begin{align}
	\mathcal M = L_{u,h}^{-1}(L_{u} - F)
\end{align}
and let $\mathcal M_h:V_h \rightarrow V_h$ be its restriction to $V_h$. Clearly, $\mathcal M_h$ reduces to 
\begin{align}
\mathcal M = id_h - L_{u,h}^{-1}F
\end{align}
Since $L_{u,h}$ is a isomorphism the every fixed point of $\mathcal M_h$ is also a root of $F$ and hence a fixed point to our Picard Iteration restriced to $V_h$.

We want to apply a contraction property as well as a mapping property to apply Banach's fixed point theorem. If we are able to show that $\M_h$ restricted to an arbitrary small set is a self-mapping we can locate the fixed point with an arbitrary precision. As Brenner et alt. we choose this set to be a small ball centered around $u_{c,h}\in V_h$, where
\[
u_{c_h} = L_{u,h}^{-1} L_u u.
\]

By \eqref{eq: add our method} for all $w \in V$
\begin{align}
	\mathcal M w &= L_{u,h}^{-1}(L_u w - Fw) \nonumber\\
				 &= L_{u,h}^{-1}(L_u w - F(u+w-u) \nonumber\\
				 &= L_{u,h}^{-1}(L_u w - Fu - L_{u^i_h} (w-u)) \nonumber\\
				 &=  L_{u,h}^{-1}(L_u w - L_{u^i_h} w + L_{u^i_h}u - L_u u + L_u u - Fu) \nonumber\\
				 & = L_{u,h}^{-1} L_u u + L_{u,h}^{-1}(L_u w - L_{u^i_h} w + L_{u^i_h}u - L_u u - Fu) \label{eq: expand M}
 \end{align}
and hence for $w_1, w_2 \in V$
\begin{align*}
	\eNorm{\mathcal M w_1 - \mathcal M w_2} =& \eNorm{L_{u,h}^{-1}(L_u (w_1-w_2) - L_{u^i_h} (w_1-w_2)}.
\end{align*}
Applying Theorem \ref{thm: SIPG stability} and Lemma \ref{la: L dependence paramter} yields the following contraction result.
\begin{lemma}[Contraction property of $\mathcal{M}$] \label{la: contraction property M}
	For any $w_1, w_2 \in V$ it holds
\begin{align*}
	\HOneDnorm{\mathcal M w_1 - \mathcal M w_2}	\leq C \epsilonTwo{u - u^i_h} \label{eq: estimate M}
\end{align*}
\end{lemma}
It is easy to see that the contraction property of $\mathcal{M}$ holds on the approximation quality of the last step's solution. %If this estimation is sharp it also means that 

To justify the choice of $u_{c,h}$ we show $u_{c,h}$ is close to the exact \MA solution.

\begin{lemma}\label{la: difference u uch}
	The distance between the exact solution $u$ and $u_{c,h}$ is mainly determined by the approximation properties of $V_h$, namely
	\[
		\eNorm{u-u_{c,h}} \leq C \inf_{v_h \in V_h} \eNorm{u-v_h}.
	\]
\end{lemma}
\begin{proof}
	For any $v_h \in V_h$ holds by applying both statements of Lemma \ref{la: stability L}
	\begin{align}
		\eNorm{u-u_{c,h}} \leq& \eNorm{u- v_h} + \eNorm{L_{u,h}^{-1}L_{u,h} v_h- L_{u,h}^{-1}L_u u} \nonumber \\
						  \leq& \eNorm{u- v_h} + C \HMinusOneDnorm{L_{u,h} v_h-  L_u u} \nonumber \\
						  \leq& \eNorm{u- v_h} + C \eNorm{v_h -u} \nonumber
	\end{align}
	Since $v_h \in V_h$ was arbitrary the desired estimate follows.
\end{proof}

\begin{lemma}[Mapping Property of $\M$] \label{la: mapping property of M}
	Let 
	\[
		\mathbb B_\rho(u_{c,h}):=\{v_h \in V_h: \eNorm{v_h - u_{c,h}} \leq \rho\}
	\]
	denote a small ball in $V_h$ centered at $u_{c,h}$. 
	Then, there holds for any $v \in \mathbb B_\rho(u_{c,h})$
	\[
		\eNorm{u_{c,h} - \M_h v} \leq C \epsilonTwo{u - u^i_h} (\rho+ \inf_{v_h \in V_h} \eNorm{u-v_h}+1)
	\] 
\end{lemma}
\begin{proof}
By \eqref{eq: add our method} for all $w \in V$
\begin{align}
	\mathcal M w &= L_{u,h}^{-1}(L_u w - Fw) \nonumber\\
				 &= L_{u,h}^{-1}(L_u w - F(u+w-u) \nonumber\\
				 &= L_{u,h}^{-1}(L_u w - Fu - L_{u^i_h} (w-u)) \nonumber\\
				 &=  L_{u,h}^{-1}(L_u w - L_{u^i_h} w + L_{u^i_h}u - L_u u + L_u u - Fu) \nonumber\\
				 & = L_{u,h}^{-1} L_u u + L_{u,h}^{-1}(L_u w - L_{u^i_h} w + L_{u^i_h}u - L_u u - Fu) \label{eq: expand M}
 \end{align}	
 and hence we have for any $w \in V$
	\begin{align}
		\eNorm{u_{c,h} - \M_h v} = \eNorm{L_{u,h}^{-1}(L_u w - L_{u^i_h} w + L_{u^i_h}u - L_u u - Fu)}
	\end{align}
	and hence using Theorem \ref{thm: SIPG stability} and \ref{la: L dependence paramter}
	\begin{align*}
		\eNorm{u_{c,h} - \M_h w} 
		\leq& C\HMinusOneDnorm{(L_u w - L_{u^i_h} w + L_{u^i_h} u- L_u u - Fu)} \nonumber\\
		\leq& C\HMinusOneDnorm{(L_u (w-u) - L_{u^i_h} (w-u) - Fu)} \nonumber\\
				\leq& C \epsilonTwo{u - u^i_h} (\eNorm{w-u}) +\HMinusOneDnorm{Fu}.
	\end{align*}
	Using the result in Lemma \ref{la: consistency error F} we obtain
	\begin{align*}
		\eNorm{u_{c,h} - \M_h w} \leq C \epsilonTwo{u - u^i_h} (\eNorm{w-u}+1).
	\end{align*}
	Consider now $v_h \in B_\rho(u_{c,h})$, by the triangle inequality we find 
	\begin{align}
		\eNorm{u_{c,h} - \M_h v} 
			\leq& C \epsilonTwo{u - u^i_h} (\eNorm{v_h-u_{c,h}}+ \eNorm{u_{c,h}-u} +1) \nonumber \\
			\leq& C \epsilonTwo{u - u^i_h} (\rho+ \eNorm{u_{c,h}-u} +1)			
	\end{align}
%	and using the estimate of Lemma \ref{la: difference u uch} implies the claimed estimate.
\end{proof}

Now we are able to combine the preceding results and thereby prove the main result
\begin{theorem}\label{main result}
	Given a sufficiently large $\sigma$ and $u_h^i$ sufficiently close to the exact solution, there exists a solution $u_h$ to the penalty method \eqref{eq: fixed point functional} satisfying
	\begin{align}
		\eNorm{u-u_h} \leq C h^{s-1} \norm{u}_{H^{s}(\Omega)} \label{eq: error estimate}
	\end{align}
	where $0 \leq s \leq \min\{k+1, t\}$ for $u$ contained in $H^t(\Omega)$. Hereby $C$ denotes a positive constant depending on the mesh, $\sigma$ and $u^i_h$, but not on $u$ and $h$.\\
	Furthermore if
	\[
		\epsilonTwo{u-u_h^i} \leq C^* h^{s} \norm{u}_{H^s(\Omega)}
	\]	
	there exist a $h^*> 0$ such that $u_h^i$ is sufficiently close to $u$ for all $h > h^*$.
	
\end{theorem}
\begin{proof}
	Fix a $h$ and take
	\begin{align}
		\rho_0:= h^{s-1} \norm{u}_{H^{s}(\Omega)}. \label{eq: definition rho0}
	\end{align}
	
	Define 
	\begin{align}
		\tau_1 &= 2 C_1 \epsilonTwo{u-u_h^i}, \label{eq: tau1}\\
		\tau_2 & = 2 C_2 \epsilonTwo{u-u_h^i}, \label{eq: tau2}\\
		\tau_3 & = C_2 \epsilonTwo{u-u_h^i} \left(\inf_{v_h \in V_h} \eNorm{u-v_h}+1\right) 2 \rho_0^{-1} \label{eq: tau3},
	\end{align}
	where $C_1$ is the constant of Lemma \ref{la: contraction property M} and $C_2$ the constant of Lemma \ref{la: mapping property of M}. 
	
	Let $u_h^i$ suffice
	\begin{align}
		%\max
		\delta := \maxTriple {\tau_1} {\tau_2} {\tau_3}< 1,
	\end{align}
	Thereby it follows from \eqref{eq: tau1} that $M$ fulfills the contraction property stated in Lemma \ref{la: contraction property M} with the constant $\delta<1$.
	
	Lemma \ref{la: mapping property of M} implies for any $v_h \in \mathbb{B}_{\rho_0}(u_{c,h})$
	 \[
	 	\eNorm{u_{c,h} - M_h v_h} \leq 
	 		C_2 \epsilonTwo{u - u^i_h} (\rho+ \inf_{v_h \in V_h} \eNorm{u-v_h}+1) 
	 \]
	By \eqref{eq: tau2} we have 
	 \[
	 	\eNorm{u_{c,h} - M_h v_h} \leq 
	 		\frac \delta 2 \rho_0 
	 			+ C_2  \epsilonTwo{u - u^i_h} \left( \inf_{v_h \in V_h} \eNorm{u-v_h}+1\right)
	 \]
	 and then by \eqref{eq: tau3} we obtain
	 \[
	 	\eNorm{u_{c,h} - M_h v_h} \leq 
	 		\frac \delta 2 \rho_0 
	 		+ \frac {\rho_0} 2  < \rho_0.
	 \]
	 			 	  		 
	 Using the Banach fixed point theorem yields the existence of a unique fixed point $u_h$ of $\M_h$ in $\mathbb B_{\rho_0}$.
	 Moreover, by Lemma \ref{la: difference u uch} it holds
	 \begin{align}
	 	\eNorm{u-u_h} \leq \eNorm{u-u_ch} + \eNorm{u_ch - u_h} \leq \inf_{v_h \in V_h} \eNorm{u-v_h} +\rho_0 
	\end{align}
	\eqref{eq: error estimate} then follows by the approximation property given in Lemma \ref{la: approximation properties}.
	
	Given $\epsilonTwo{u-u_h^i} \leq h^{s+1} \norm{u}_H^s(\Omega)$ we find using the definitions of $\rho$ and $\tau_3$ (cf. \eqref{eq: definition rho0} and \eqref{eq: tau3}
	\begin{align*}
		\tau_3  \leq C h^{s} \norm{u}_{H^s(\Omega)} \left(h^{s-1} \norm{u}_H^s(\Omega)+1\right) h^{-s+1} \norm{u}_{H^{s}(\Omega)}^{-1} \xrightarrow{h \rightarrow 0}  0.
	\end{align*}
	Since analogously $\tau_1, \tau_2 \xrightarrow[]{h\rightarrow 0} 0$ holds there exist a $h^*$ such that their maximum, i.e. $\delta$ is smaller than 1.
\end{proof}
	Note, that $\mathcal P^k_h$ for $u \in H^t(\Omega)$ has the approximation property 
	\[
		\inf_{v \in V_h} \sum_{T \in \triang} \singleNorm{u-v_h}_{H^2(\Omega)} \leq h^{l-2} \singleNorm{u}_{H^l(\Omega)},
	\]
	where $l = \min\{k+1,t\}$.
	Thus, in general we have no evidence that \eqref{eq: error estimate} holds for $s$ greater than $l-2$ even our previous guess $u_h^i$ is very good.
